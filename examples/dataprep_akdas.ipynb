{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ad34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from obspy import UTCDateTime\n",
    "from obspy.taup import TauPyModel\n",
    "from obspy.core.event import Catalog\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.geodetics.base import locations2degrees, degrees2kilometers\n",
    "\n",
    "from das_util import next_power_of_2\n",
    "from das_util import fk_filter_2cones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "124f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ak_catalog(t1, t2, lat0=59.86, lon0=-151.85, a=-1, b=0.65):\n",
    "    '''\n",
    "    In:  t1, t2: start and ending timestamps\n",
    "         lat0,lon0: Reference point of DAS network\n",
    "         a, b: simple GMM parameters\n",
    "    Out: cat : USGS AK catalog meeting GMM threshold\n",
    "         ptimes : absolute P arrival times\n",
    "    '''\n",
    "    events = []\n",
    "    ptimes = []\n",
    "\n",
    "    # Get local catalog\n",
    "    catalog = Client('IRIS').get_events(\n",
    "        starttime=t1,\n",
    "        endtime=t2,\n",
    "#        catalog='ak',\n",
    "        includeallorigins=True,\n",
    "        includeallmagnitudes=True)\n",
    "\n",
    "    catalog.write(\"example.xml\", format=\"QUAKEML\")\n",
    "    for event in catalog:\n",
    "        lon = event.origins[0]['longitude']\n",
    "        lat = event.origins[0]['latitude']\n",
    "        dep = event.origins[0]['depth'] * 1e-3\n",
    "        mag = event.magnitudes[0]['mag']\n",
    "        distdeg = locations2degrees(lat0, lon0, lat, lon)\n",
    "        distkm = degrees2kilometers(distdeg)\n",
    "        rad = np.sqrt(distkm ** 2 + dep ** 2)\n",
    "\n",
    "        if (mag - 10 ** (a + b * np.log10(rad)) >= 0):\n",
    "            model = TauPyModel(model='iasp91')\n",
    "            arr = model.get_travel_times(\n",
    "                source_depth_in_km=dep,\n",
    "                distance_in_degree=distdeg)\n",
    "\n",
    "            t0 = event.origins[0]['time']\n",
    "            ptimes.append(t0 + arr[0].time)\n",
    "            events.append(event)\n",
    "\n",
    "    return Catalog(events=events), np.array(ptimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4992d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ak_record_lists(rec_dir, format_part, format_full, times, catalog):\n",
    "    '''\n",
    "    In:  rec_dir : path to the raw data\n",
    "         format_part/full: file name format\n",
    "         times: event first arrival time\n",
    "    Out: elist : list of files records events\n",
    "         nlist : list of files of noises\n",
    "    '''\n",
    "    elist = []\n",
    "    nlist = []\n",
    "    events = []\n",
    "\n",
    "    for t_arrival, eve in zip(times, catalog):\n",
    "        fname = UTCDateTime.strftime(t_arrival, format=format_part)\n",
    "        print(rec_dir + fname)\n",
    "        try:\n",
    "            fname = os.path.basename(glob.glob(rec_dir + fname)[0])\n",
    "        except:\n",
    "            continue\n",
    "        t_file = UTCDateTime.strptime(fname, format=format_full)\n",
    "        if (t_arrival - t_file) > 0:\n",
    "            t_eq = t_file\n",
    "        else:\n",
    "            t_eq = t_file - 60\n",
    "        t_no = t_eq - 60\n",
    "\n",
    "        fname = UTCDateTime.strftime(t_eq, format=format_part)\n",
    "        eq_file = os.path.basename(glob.glob(rec_dir + fname)[0])\n",
    "        fname = UTCDateTime.strftime(t_no, format=format_part)\n",
    "        no_file = os.path.basename(glob.glob(rec_dir + fname)[0])\n",
    "\n",
    "        elist.append(os.path.join(rec_dir, eq_file))\n",
    "        nlist.append(os.path.join(rec_dir, no_file))\n",
    "        events.append(eve)\n",
    "\n",
    "    return elist, nlist, Catalog(events=events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1ebca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taper_axis2(arr):\n",
    "    m, n = arr.shape\n",
    "    taper_window = np.hanning(n)\n",
    "    tapered_arr = arr * taper_window\n",
    "\n",
    "    return tapered_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a644de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_akdas(outdir, seis_arrays, rec_dirs, format_part, format_full, times, catalog):\n",
    "    for rec_dir, seis_array, f_part, f_full in zip(rec_dirs, seis_arrays, format_part, format_full):\n",
    "        elist, nlist, cat = ak_record_lists(rec_dir, f_part, f_full, times, catalog)\n",
    "        print(len(cat), len(elist))\n",
    "        cat.write(\"ak_2024Jan.xml\", format=\"QUAKEML\")\n",
    "        # if not len(elist) == len(nlist):\n",
    "        #     print('Inconsistent number of quake and noise files')\n",
    "        #     raise ValueError\n",
    "\n",
    "        all_quake = np.zeros((len(elist), 7500, 1500), dtype=np.float32)\n",
    "        # all_noise = np.zeros((len(nlist), 7500, 1500), dtype=np.float32)\n",
    "        raw_quake = np.zeros((len(elist), 7500, 1500), dtype=np.float32)\n",
    "\n",
    "        for i, (eq_file, no_file) in enumerate(zip(elist, nlist)):\n",
    "            with h5py.File(eq_file, 'r') as f:\n",
    "                time_data = f['Acquisition']['Raw[0]']['RawData'][:1500, 100:7600]\n",
    "\n",
    "            time_data = (time_data - np.mean(time_data)) / np.std(time_data)\n",
    "\n",
    "            raw_quake[i, :, :time_data.shape[0]] = time_data.T\n",
    "\n",
    "            # %% Use FK filter\n",
    "            filt_cplx, mask_fk, fk2d = fk_filter_2cones(time_data,\n",
    "                                                        w1=0.005,\n",
    "                                                        w2=0.25,\n",
    "                                                        cone1=True,\n",
    "                                                        cone2=True)\n",
    "            time_data = filt_cplx.real\n",
    "\n",
    "            all_quake[i, :, :time_data.shape[0]] = time_data.T\n",
    "\n",
    "        today = UTCDateTime.strftime(UTCDateTime.now(), format='%Y_%m_%d')\n",
    "        with h5py.File(outdir + '/' + seis_array + 'till' + today + '.hdf5', 'w') as f:\n",
    "            f.create_dataset(\"fk_quake\", data=all_quake)\n",
    "            f.create_dataset(\"raw_quake\", data=raw_quake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## main function to generate training dataset\n",
    "seis_arrays = ['KKFLS', 'TERRA']\n",
    "rec_dirs = ['/mnt/qnap/KKFL-S_FIberA_25Hz/', '/mnt/qnap/TERRA_FiberA_25Hz/']\n",
    "# seis_arrays = ['TERRA']\n",
    "# rec_dirs = ['/mnt/qnap/TERRA_FiberA_25Hz/']\n",
    "format_part = ['decimator2_%Y-%m-%d_%H.%M.??_UTC.h5', 'decimator2_%Y-%m-%d_%H.%M.??_UTC.h5']\n",
    "format_full = ['decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5', 'decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5']\n",
    "outdir = '/mnt/disk2/qibin_data'\n",
    "\n",
    "t1 = UTCDateTime(\"2023-07-01T00:00:00\")\n",
    "t2 = UTCDateTime(\"2023-07-04T00:00:00\")\n",
    "cat, ptimes = ak_catalog(t1, t2, a=-1.2, b=0.65)\n",
    "dataprep_akdas(outdir, seis_arrays, rec_dirs, format_part, format_full, ptimes, cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
