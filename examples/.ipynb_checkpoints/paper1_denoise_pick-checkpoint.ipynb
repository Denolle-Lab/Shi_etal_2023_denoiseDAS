{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf73e7fe",
   "metadata": {},
   "source": [
    "# Enhanced Offshore Earthquake Monitoring by DAS Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d7ccf",
   "metadata": {},
   "source": [
    "**Authors: Qibin Shi (qibins@uw.edu/ qshi003@e.ntu.edu.sg), Marine Denolle (mdenolle@uw.edu)**\n",
    "\n",
    "Comments obtained from Ethan Williams and Kuan-Fu Feng\n",
    "\n",
    "This notebook is created originally to analyze the Alaska DAS data maintained by the UWQuake research group.\n",
    "\n",
    "It has 4 sessions. Each session will save the essential results for the next steps. \n",
    "\n",
    "For optimized experience, we recommend:\n",
    "\n",
    "1. GPU\n",
    "\n",
    "2. H5 file format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c57bc",
   "metadata": {},
   "source": [
    "### Import modules before any session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6878d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/denoiser/')\n",
    "import h5py\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from obspy import read_events\n",
    "from obspy.taup import TauPyModel\n",
    "from obspy.core import UTCDateTime\n",
    "from distaz import DistAz\n",
    "from joblib import Parallel, delayed\n",
    "import scipy.signal as sgn\n",
    "from scipy.signal import filtfilt, butter\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "\n",
    "from das_util import try_gpu\n",
    "from das_denoise_models import unet\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "\n",
    "### Parameters for DAS \n",
    "sample_rate = 25\n",
    "dchan = 9.5714\n",
    "ch_max = 4500  # max channel of each cable (4500 or 6000)\n",
    "ch_itv=2  # channels are downsampled for faster picking\n",
    "\n",
    "### Directories and files\n",
    "raw_dir = '/fd1/QibinShi_data/akdas/qibin_data/'\n",
    "out_dir = raw_dir + 'largerEQ_plots_test_picking_dec_ch' + str(ch_max) + '/'\n",
    "record_time_file = 'recording_times_larger.csv'\n",
    "qml = raw_dir + 'ak_Dec1_31_a120b065.xml'\n",
    "\n",
    "\n",
    "############################ Functions for this notebook\n",
    "\n",
    "### Functions to calculate P and S time on arrays\n",
    "### Travel time for given channels\n",
    "def array_tpts(i, stla, stlo, evla, evlo, evdp):\n",
    "    \n",
    "    distdeg = DistAz(stla[i], stlo[i], evla, evlo).getDelta()\n",
    "    tp = TauPyModel(model=\"iasp91\").get_travel_times(source_depth_in_km=evdp, \n",
    "                                                     distance_in_degree=distdeg, \n",
    "                                                     phase_list=['p', 'P'])\n",
    "    ts = TauPyModel(model=\"iasp91\").get_travel_times(source_depth_in_km=evdp, \n",
    "                                                     distance_in_degree=distdeg, \n",
    "                                                     phase_list=['s', 'S'])\n",
    "    return [tp[0].time, ts[0].time]\n",
    "\n",
    "\n",
    "### Add travel time for AK DAS arrays to dataframe\n",
    "def akdas_tpts(cat, eid, kkfls, terra, correct_terra):\n",
    "    ### Event info\n",
    "    evt = cat[eid]\n",
    "    mag = evt.magnitudes[0].mag\n",
    "    lon = evt.origins[0].longitude\n",
    "    lat = evt.origins[0].latitude\n",
    "    dep = evt.origins[0].depth/1000 + 2.0  ## too shallow depth+ long distance = bugs\n",
    "    ort = evt.origins[0].time\n",
    "    \n",
    "    mag0,lon0,lat0,dep0,ort0=round(mag,1),round(lon,2),round(lat,2),round(dep,0),ort.strftime('%Y-%m-%d')\n",
    "\n",
    "    ### array info\n",
    "    t_kkfls=np.array(Parallel(n_jobs=100)(delayed(array_tpts)(ch,kkfls['lat'].values,kkfls['lon'].values,lat,lon,dep) \n",
    "                                           for ch in range(len(kkfls))))\n",
    "    t_terra=np.array(Parallel(n_jobs=100)(delayed(array_tpts)(ch,terra['lat'].values,terra['lon'].values,lat,lon,dep) \n",
    "                                           for ch in range(len(terra))))\n",
    "\n",
    "    kkfls['tp'] = t_kkfls[:, 0]\n",
    "    kkfls['ts'] = t_kkfls[:, 1]\n",
    "    terra['tp'] = t_terra[:, 0]+correct_terra\n",
    "    terra['ts'] = t_terra[:, 1]+correct_terra\n",
    "    \n",
    "    return kkfls, terra, mag0,lon0,lat0,dep0,ort0\n",
    "\n",
    "\n",
    "### Functions to denoise large-N DAS array\n",
    "def process_3d_array(arr, len1=1500, len2=1500):\n",
    "    \"\"\"convert to numpy array\"\"\"\n",
    "    arr = np.array(arr)\n",
    "    \n",
    "    \"\"\"Ensure the array has at least len1 rows and len2 columns\"\"\"\n",
    "    slices, rows, cols = arr.shape\n",
    "    arr = arr[:, :min(rows, len1), :min(cols, len2)]\n",
    "    \n",
    "    \"\"\"Pad zeros if it has fewer than len1 rows or len2 columns\"\"\"\n",
    "    if rows < len1 or cols < len2:\n",
    "        padding_rows = max(len1 - rows, 0)\n",
    "        padding_cols = max(len2 - cols, 0)\n",
    "        arr = np.pad(arr, ((0, 0), (0, padding_rows), (0, padding_cols)), 'constant')\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def Denoise_largeDAS(data, model_func, devc, repeat=4, norm_batch=False):\n",
    "    \"\"\" This function do the following (it does NOT filter data):\n",
    "    1) split into multiple 1500-channel segments\n",
    "    2) call Denoise function for each segments\n",
    "    3) merge all segments\n",
    "    \n",
    "    data: 2D -- [channel, time]\n",
    "    output: 2D, but padded 0 to have multiple of 1500 channels\n",
    "    \n",
    "    This code was primarily designed for the Alaska DAS, but applicable to other networks\n",
    "    \"\"\" \n",
    "    data = np.array(data)\n",
    "    nchan = data.shape[0]\n",
    "    ntime = data.shape[1]\n",
    "    \n",
    "    if (nchan // 1500) == 0:\n",
    "        n_seg = nchan // 1500\n",
    "    else:\n",
    "        n_seg = nchan // 1500 + 1\n",
    "        \n",
    "    full_len = int(n_seg * 1500)\n",
    "    \n",
    "    pad_data = process_3d_array(data[np.newaxis,:,:], len1=full_len)\n",
    "    data3d = pad_data.reshape((-1, 1500, 1500))\n",
    "    \n",
    "    oneDenoise, mulDenoise = Denoise(data3d, model_func, devc, repeat=repeat, norm_batch=norm_batch)\n",
    "    \n",
    "    oneDenoise2d = oneDenoise.reshape((full_len, 1500))[:nchan, :ntime]\n",
    "    mulDenoise2d = mulDenoise.reshape((full_len, 1500))[:nchan, :ntime]\n",
    "    \n",
    "    return oneDenoise2d, mulDenoise2d\n",
    "    \n",
    "\n",
    "def Denoise(data, model_func, devc, repeat=4, norm_batch=False):\n",
    "    \"\"\" This function do the following (it does NOT initialize model):\n",
    "\n",
    "    1) normalized the data\n",
    "    2) ensure the data format, precision and size\n",
    "    3) denoise and scale back the output amplitude\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\" convert to torch tensors \"\"\"\n",
    "    if norm_batch:\n",
    "        scale = np.std(data[-1]) + 1e-7  ### Avoid potentially bad beginning sub-images\n",
    "    else:\n",
    "        scale = np.std(data, axis=(1,2), keepdims=True) + 1e-7\n",
    "        \n",
    "    data_norm = data / scale  ## standard scaling\n",
    "    arr = process_3d_array(data_norm.astype(np.float32))\n",
    "    X = torch.from_numpy(arr).to(devc)\n",
    "    \n",
    "    \"\"\" denoise - deploy \"\"\"\n",
    "    with torch.no_grad():\n",
    "        oneDenoise = model_func(X)\n",
    "        mulDenoise = oneDenoise\n",
    "        \n",
    "        for i in range(repeat-1):\n",
    "            mulDenoise = model_func(mulDenoise)\n",
    "\n",
    "    \"\"\" convert back to numpy \"\"\"\n",
    "    oneDenoise = oneDenoise.to('cpu').numpy() * scale\n",
    "    mulDenoise = mulDenoise.to('cpu').numpy() * scale\n",
    "    \n",
    "    return oneDenoise[:, :len(data[0]), :len(data[0][0])], mulDenoise[:, :len(data[0]), :len(data[0][0])]\n",
    "\n",
    "def vizRawDenoise(in_data, oneDenoise, mulDenoise, sample_rate=25, dchan=10, index=[0,1], model=\"MAE\"):\n",
    "    \"\"\"\n",
    "    in_data, oneDenoise, mulDenoise: 3D -- [event, channel, time]\n",
    "    index: list, subset of the events\n",
    "    model: string, descriptions about the model\n",
    "    \"\"\"\n",
    "    len1, len2 = oneDenoise[0].shape[0], oneDenoise[0].shape[1]\n",
    "    x, y = np.arange(len2)/sample_rate, np.arange(0-len1/2, len1/2)*dchan/1000\n",
    "    rawdata = process_3d_array(in_data, len1=len1, len2=len2)\n",
    "    \n",
    "    matplotlib.rcParams['font.size'] = 20\n",
    "\n",
    "    for j in index:\n",
    "        bound = np.percentile(np.fabs(in_data[j]), 80)\n",
    "        cmp = matplotlib.colormaps['RdBu']\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "        img=ax[0].pcolormesh(x, y, rawdata[j], shading='auto', vmin=-bound, vmax=bound, cmap=cmap)\n",
    "        ax[1].pcolormesh(x, y, oneDenoise[j], shading='auto',  vmin=-bound, vmax=bound, cmap=cmap)\n",
    "        ax[2].pcolormesh(x, y, mulDenoise[j], shading='auto', vmin=-bound, vmax=bound, cmap=cmap)\n",
    "\n",
    "        ax[0].set_title(\"Raw data #\"+str(j))\n",
    "        ax[1].set_title(model+\" 1-time denoised\")\n",
    "        ax[2].set_title(model+\" multi-time denoised\")\n",
    "        ax[0].set_ylabel('Distance (km)')\n",
    "        ax[0].set_xlabel('Time (s)')\n",
    "        ax[1].set_xlabel('Time (s)')\n",
    "        ax[2].set_xlabel('Time (s)')\n",
    "\n",
    "        plt.colorbar(img, ax=ax[2], aspect=50)\n",
    "\n",
    "\n",
    "### Functions to pick large DAS arrays\n",
    "def process_p(ista,paras_semblance,batch_pred,istart,sfs):\n",
    "    \n",
    "        crap = ensemble_semblance(batch_pred[:, ista, :], paras_semblance)\n",
    "        imax = np.argmax(crap[istart:])\n",
    "            \n",
    "        return float((imax)/sfs)+istart/sfs, crap[istart+imax]\n",
    "    \n",
    "\n",
    "def apply_elep(DAS_data, list_models, fs, paras_semblance, device):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    This function takes a array of stream, a list of ML models and \n",
    "    apply these models to the data, predict phase picks, and\n",
    "    return an array of picks .\n",
    "    DAS_data: NDArray of DAS data: [channel,time stamp - 6000]\n",
    "    \"\"\"\n",
    "    \n",
    "    twin = 6000  ## needed by EQTransformer\n",
    "    nsta = DAS_data.shape[0]\n",
    "    bigS = np.zeros(shape=(DAS_data.shape[0], 3, DAS_data.shape[1]))\n",
    "    for i in range(nsta):   ### same data copied to three components\n",
    "        bigS[i,0,:] = DAS_data[i,:]\n",
    "        bigS[i,1,:] = DAS_data[i,:]\n",
    "        bigS[i,2,:] = DAS_data[i,:]\n",
    "\n",
    "    # allocating memory for the ensemble predictions\n",
    "    batch_pred_P = np.zeros(shape=(len(list_models),nsta,twin)) \n",
    "    batch_pred_S = np.zeros(shape=(len(list_models),nsta,twin))\n",
    "        \n",
    "    ######### Broadband workflow ################\n",
    "    crap2 = bigS.copy()\n",
    "    crap2 -= np.mean(crap2, axis=-1, keepdims= True) # demean data\n",
    "    # original use std norm\n",
    "    data_std = crap2 / (np.std(crap2) + 1e-7)\n",
    "    # could use max data\n",
    "    mmax = np.max(np.abs(crap2), axis=-1, keepdims=True)\n",
    "    data_max = np.divide(crap2 , mmax,out=np.zeros_like(crap2), where=mmax!=0)\n",
    "    del bigS\n",
    "    \n",
    "    # data to tensor\n",
    "    data_tt = torch.from_numpy(data_max).to(device, dtype=torch.float64)\n",
    "    \n",
    "    for ii, imodel in enumerate(list_models):\n",
    "        imodel.to(device)\n",
    "        imodel=imodel.double()\n",
    "        imodel.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_pred_P[ii, :, :] = imodel(data_tt)[1].cpu().numpy()[:, :]\n",
    "            batch_pred_S[ii, :, :] = imodel(data_tt)[2].cpu().numpy()[:, :]\n",
    "    \n",
    "    smb_peak = np.zeros([nsta,2,2], dtype = np.float32)\n",
    "\n",
    "    smb_peak[:,0,:] =np.array(Parallel(n_jobs=100)(delayed(process_p)(ista,paras_semblance,batch_pred_P,0,fs) \n",
    "                                                    for ista in range(nsta)))\n",
    "    smb_peak[:,1,:] =np.array(Parallel(n_jobs=100)(delayed(process_p)(ista,paras_semblance,batch_pred_S,0,fs) \n",
    "                                                    for ista in range(nsta)))\n",
    "    \n",
    "    return smb_peak\n",
    "\n",
    "\n",
    "### plotting codes to simplify massive event processing\n",
    "\n",
    "### plot picked times\n",
    "def subfig_img(image, pick, ind_p, ind_s, pred, array, colors=['blue', 'green']):\n",
    "    x = np.arange(image.shape[1])/fs\n",
    "    y = np.arange(0-image.shape[0]/2,image.shape[0]/2)*ch_itv*dchan/1000\n",
    "    bound = np.percentile(np.fabs(image), 80)\n",
    "    \n",
    "    plt.pcolormesh(x,y,image, shading='auto',vmin=-bound,vmax=bound,cmap=matplotlib.colormaps['RdBu'])\n",
    "    plt.plot(pred[:,1], array, color='red', linestyle='-', lw=5) \n",
    "    # plt.plot(pred[:,0], array, color='orange', linestyle='-', lw=5)   \n",
    "    # plt.scatter(pick[ind_p,0,0],y[ind_p], s=15,marker='o',c=colors[0],alpha=0.3)\n",
    "    plt.scatter(pick[ind_s,1,0],y[ind_s], s=15,marker='o',c=colors[1],alpha=0.3)\n",
    "    plt.ylabel(\"Distance along cable (km)\")\n",
    "\n",
    "### plot picking likelihood\n",
    "def subfig_histpick(pick, colors=['blue', 'green'], labels=['P', 'S']):\n",
    "    plt.hist(pick[:,0,1],bins=20,color=colors[0],label=labels[0],range=(0,0.3))\n",
    "    plt.hist(pick[:,1,1],bins=20,color=colors[1],label=labels[1],range=(0,0.3))\n",
    "    plt.title(\"Picks count\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylim(0,500)\n",
    "    plt.xlim(0.05,0.3)\n",
    "\n",
    "### plot waveforms\n",
    "def subfig_goodtrace(image, pick, ind_p, ind_s, tax, win):\n",
    "    snr_p=0\n",
    "    snr_s=0\n",
    "    \n",
    "    for ch in ind_p:\n",
    "        pt=int(pick[ch,0,0]*fs)\n",
    "        snr_p+=np.std(image[ch, pt:pt+fs]) / (np.std(image[ch, pt-fs:pt])+1e-7)\n",
    "    for ch in ind_s: \n",
    "        pt=int(pick[ch,1,0]*fs)\n",
    "        snr_s+=np.std(image[ch, pt:pt+fs]) / (np.std(image[ch, pt-fs:pt])+1e-7)\n",
    "        plt.plot(tax[win], image[ch, win])\n",
    "            \n",
    "    return snr_p/(len(ind_p) + 1e-7), snr_s/ (len(ind_s) + 1e-7)\n",
    "\n",
    "\n",
    "def fit_series(s1, s2, prob, thr=0.05, vmin=0, vmax=60):\n",
    "    offsets = s1-s2\n",
    "    ind = np.where(np.logical_and(np.logical_and(np.logical_and(vmin<s1, s1<vmax), prob > thr), np.fabs(offsets) < 3.0))[0]\n",
    "    \n",
    "    if len(ind)>0:\n",
    "        offsets = offsets[ind]\n",
    "    else:\n",
    "        offsets = 0\n",
    "        \n",
    "    mean_offset = np.mean(offsets)\n",
    "    offsets = offsets-mean_offset\n",
    "    \n",
    "    return offsets, round(np.std(offsets),3), mean_offset, ind\n",
    "\n",
    "\n",
    "### Functions for CC-based alignment\n",
    "def shift2maxcc(wave1, wave2, maxshift=5):\n",
    "    n1 = np.sum(np.square(wave1))\n",
    "    n2 = np.sum(np.square(wave2))\n",
    "    corr = sgn.correlate(wave1, wave2) / np.sqrt(n1 * n2)\n",
    "    lags = sgn.correlation_lags(len(wave1), len(wave2))\n",
    "\n",
    "    st_pt = len(wave2) - min(len(wave2), maxshift)\n",
    "    en_pt = len(wave2) + min(len(wave1), maxshift)\n",
    "\n",
    "    ind1 = np.argmax(corr[st_pt: en_pt]) + st_pt\n",
    "\n",
    "    return lags[ind1], corr[ind1]\n",
    "\n",
    "\n",
    "def shift_pad(wave, shift_pt=0):\n",
    "    tmp_tr = np.zeros(wave.shape, dtype=np.float32)\n",
    "    if shift_pt > 0:\n",
    "        tmp_tr[shift_pt:] = wave[0:0 - shift_pt]\n",
    "    elif shift_pt < 0:\n",
    "        tmp_tr[0:shift_pt] = wave[0 - shift_pt:]\n",
    "    else:\n",
    "        tmp_tr[:] = wave[:]\n",
    "    return tmp_tr\n",
    "\n",
    "\n",
    "### Functions to shift each channel of the 2D data using shift2maxcc\n",
    "def align_channels_twice(data, ref_ch=0, maxshift=5, cc_thres=0.5):\n",
    "\n",
    "    nchan, ntime = data.shape\n",
    "\n",
    "    aligned = np.zeros_like(data)\n",
    "    shifts = np.zeros(nchan)\n",
    "    cccs = np.zeros(nchan)\n",
    "\n",
    "    ## First round of alignment with reference channel\n",
    "    count = 0\n",
    "    ref = np.zeros(ntime)\n",
    "    for i in range(nchan):\n",
    "        shift, ccc = shift2maxcc(data[ref_ch], data[i], maxshift=maxshift)\n",
    "        if ccc > cc_thres:\n",
    "            ref += shift_pad(data[i], shift_pt=int(shift))\n",
    "            count += 1\n",
    "    ref /= count\n",
    "\n",
    "    ## second round of alignment with the stacked aligned data\n",
    "    for i in range(nchan):\n",
    "        shifts[i], cccs[i] = shift2maxcc(ref, data[i], maxshift=maxshift)\n",
    "        aligned[i,:] = shift_pad(data[i], shift_pt=int(shifts[i]))\n",
    "\n",
    "    return aligned, shifts, cccs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa48bce",
   "metadata": {},
   "source": [
    "Doing some tests...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395effc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with h5py.File('/fd1/QibinShi_data/akdas/qibin_data/Dec2023/kkfls_2023-12-02_08.30.57_UTC.h5', 'r') as f:\n",
    "    timestamp_ffkls = f['Acquisition']['Raw[0]']['RawDataTime'][:]\n",
    "    gauge_len = f['Acquisition'].attrs['GaugeLength']\n",
    "    attrs_raw0 = dict(f['Acquisition']['Raw[0]'].attrs)\n",
    "    attrs = dict(f['Acquisition'].attrs)\n",
    "\n",
    "with h5py.File('/fd1/QibinShi_data/akdas/qibin_data/Dec2023/terra_2023-12-02_08.30.56_UTC.h5', 'r') as f:\n",
    "    timestamp_terra = f['Acquisition']['Raw[0]']['RawDataTime'][:]\n",
    "\n",
    "times_ffkls=[datetime.datetime.utcfromtimestamp(time1/1000000) for time1 in timestamp_ffkls]\n",
    "times_terra=[datetime.datetime.utcfromtimestamp(time1/1000000) for time1 in timestamp_terra]\n",
    "\n",
    "## get the difference in start time of two fibers\n",
    "terra_early = (timestamp_ffkls[0] - timestamp_terra[0]) / 1e6\n",
    "terra_early_pt = int(terra_early * 25)\n",
    "terra_early_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b60a4c",
   "metadata": {},
   "source": [
    "## 1. How The Raw DAS Look Like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad743a01",
   "metadata": {},
   "source": [
    "Where is the data? How big?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66481fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47G\t/fd1/QibinShi_data/akdas/qibin_data/KKFLStill2024_02_16.hdf5\n",
      "8.0G\t/fd1/QibinShi_data/akdas/qibin_data/KKFLStill2024_02_24.hdf5\n",
      "47G\t/fd1/QibinShi_data/akdas/qibin_data/TERRAtill2024_02_16.hdf5\n",
      "8.0G\t/fd1/QibinShi_data/akdas/qibin_data/TERRAtill2024_02_24.hdf5\n"
     ]
    }
   ],
   "source": [
    "! du -sh {raw_dir+\"*till2024_02*\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce92f5",
   "metadata": {},
   "source": [
    "### Read DAS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806ae000",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we merge two DAS cables\n",
    "### Make sure your data has complete samples in space and time\n",
    "data_terra = raw_dir + 'TERRAtill2024_02_24.hdf5'\n",
    "data_kkfls = raw_dir + 'KKFLStill2024_02_24.hdf5'\n",
    "with h5py.File(data_terra, 'r') as f:\n",
    "    quake1 = f['raw_quake'][:,:ch_max,:]  \n",
    "with h5py.File(data_kkfls, 'r') as f:\n",
    "    quake2 = f['raw_quake'][:,:ch_max,:]\n",
    "    \n",
    "### concatenate cable 1 and cable 2 along channels\n",
    "rawdata = np.append(quake2[:, ::-1, :], quake1[:,:,:], axis=1)\n",
    "rawdata = np.nan_to_num(rawdata)\n",
    "\n",
    "### Bandpass filter\n",
    "b, a = butter(4, (0.5, 12), fs=sample_rate, btype='bandpass')\n",
    "filt = filtfilt(b, a, rawdata, axis=2)\n",
    "rawdata = filt / np.std(filt, axis=(1,2), keepdims=True)  ## Rawdata w.r.t. Denoised "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69996757",
   "metadata": {},
   "source": [
    "### Information about earthquakes (metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996826e0",
   "metadata": {},
   "source": [
    "What do we know about the earthquake?\n",
    "1. Magnitude\n",
    "2. Location\n",
    "3. Origin time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41308f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 records of metadata\n",
      "95 earthquake images\n"
     ]
    }
   ],
   "source": [
    "cat = read_events(qml)\n",
    "\n",
    "print(len(cat), 'records of metadata')\n",
    "print(len(rawdata), 'earthquake images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9205c",
   "metadata": {},
   "source": [
    "### Predict the P and S time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e23afdc",
   "metadata": {},
   "source": [
    "What information do we need?\n",
    "1. Cable locations  -- relative times/ moveout\n",
    "2. Recording time  -- absolute times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6f0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cable coordinates from Ethan Williams\n",
    "kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "\n",
    "### calculate the along-cable distance from reference channels\n",
    "### Here the 500th channels of both cables are reference points\n",
    "kkfls = kkfls[(kkfls['cha']>499) & (kkfls['cha']<(500+ch_max))]\n",
    "terra = terra[(terra['cha']>499) & (terra['cha']<(500+ch_max))]\n",
    "kkfls['dist'] = (500 - kkfls['cha']) * dchan\n",
    "terra['dist'] = (terra['cha'] - 500) * dchan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9578f",
   "metadata": {},
   "source": [
    "### Plot signals and phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05faae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time is calculated\n",
      "plotting\n",
      "fig1\n",
      "fig2\n",
      "fig done\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "### predict phases for a given event in the catalog\n",
    "event_number = 11\n",
    "kkfls, terra, mag0,lon0,lat0,dep0,ort0 = akdas_tpts(cat, event_number, kkfls, terra, terra_early)\n",
    "\n",
    "print(\"time is calculated\")\n",
    "### Shift for recording time\n",
    "df = pd.read_csv(raw_dir + record_time_file).iloc[event_number]\n",
    "start_times = df['record_time']\n",
    "shift_time = cat[event_number].origins[0].time - UTCDateTime.strptime(start_times, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') \n",
    "            \n",
    "\n",
    "### plot\n",
    "time_data = rawdata[event_number]\n",
    "# time_data = mul_denoised[event_number]\n",
    "\n",
    "print(\"plotting\")\n",
    "x = np.arange(time_data.shape[1])/sample_rate\n",
    "y = np.arange(0-time_data.shape[0]/2, time_data.shape[0]/2)*dchan/1000\n",
    "cmap=matplotlib.colormaps['RdBu']\n",
    "bound = np.percentile(np.fabs(rawdata[event_number]), 80)/1.3\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7), constrained_layout=True,\n",
    "                      gridspec_kw={'width_ratios': [1, 3], 'wspace': 0.1})\n",
    "plt.title(f'Event M{mag0}   [{lat0}, {lon0}, {dep0}]   {ort0}',fontsize=22)\n",
    "\n",
    "### full array\n",
    "print(\"fig1\")\n",
    "img=ax[0].pcolormesh(x, y, time_data, shading='auto', vmin=-bound, vmax=bound, cmap=cmap)\n",
    "ax[0].plot(kkfls['ts']+shift_time, kkfls['dist']/1000, color='green', linestyle='-', lw=1)\n",
    "ax[0].plot(terra['ts']+shift_time, terra['dist']/1000, color='green', linestyle='-', lw=1)\n",
    "ax[0].set_xlabel(\"Time (s)\", fontsize=20); \n",
    "ax[0].set_ylabel(\"Distance (km)\", fontsize=20)\n",
    "\n",
    "### sub array\n",
    "print(\"fig2\")\n",
    "ch1=ch_max\n",
    "ch2=ch_max + 1500\n",
    "y=y[ch1:ch2]\n",
    "time_data=time_data[ch1:ch2]\n",
    "ax[0].plot([x[5],x[5],x[-5],x[-5],x[5]],\n",
    "           [y[0],y[-1],y[-1],y[0],y[0]], \n",
    "           color='orange', linestyle='-', lw=5)\n",
    "\n",
    "img=ax[1].pcolormesh(x, y, time_data, shading='auto', vmin=-bound, vmax=bound, cmap=cmap)\n",
    "ax[1].plot(terra['ts'].iloc[0:1500]+shift_time, terra['dist'].iloc[0:1500]/1000, color='green', linestyle='--', lw=1)\n",
    "# cbr=plt.colorbar(img, aspect=50, ax=ax[1]); cbr.set_label('amplitude', fontsize = 20)\n",
    "ax[1].set_xlabel(\"Time (s)\", fontsize=20)\n",
    "ax[1].set_xlim(15, 25)\n",
    "print(\"fig done\")\n",
    "# plt.savefig('figure.pdf', format='pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99782fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()\n",
    "# plt.savefig(out_dir + 'event_' + str(event_number) + '_muldenoised.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104be99",
   "metadata": {},
   "source": [
    "## 2. How Does The Denoiser Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41489700",
   "metadata": {},
   "source": [
    "It is classic U-net trained on our DAS recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize the U-net model \"\"\"\n",
    "devc = try_gpu(i=1)\n",
    "\n",
    "model_1 = unet(1, 16, 1024, factors=(5, 3, 2, 2), use_att=False)\n",
    "model_1 = nn.DataParallel(model_1, device_ids=[1,2,3])\n",
    "model_1.to(devc)\n",
    "\n",
    "\"\"\" Load the pretrained weights \"\"\"\n",
    "model_1.load_state_dict(torch.load('../models/checkpoint_noatt_LRdecays0.8_mask0.5_raw2raw_chmax4500.pt'))  # raw2raw\n",
    "model_1.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aacfcf",
   "metadata": {},
   "source": [
    "Denoise a small sample DAS (you can skip this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0976d2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X=rawdata[:100,:,:].astype(np.float32)\n",
    "X=rawdata[:,:,:].astype(np.float32)\n",
    "\n",
    "X = torch.from_numpy(X).to(devc)\n",
    "### denoise\n",
    "with torch.no_grad():\n",
    "    ### raw2raw\n",
    "    oneDenoise_1 = model_1(X)\n",
    "    mulDenoise_1 = model_1(oneDenoise_1)\n",
    "    mulDenoise_1 = model_1(mulDenoise_1)\n",
    "    mulDenoise_1 = model_1(mulDenoise_1)\n",
    "\n",
    "### convert back to numpy, trim edges\n",
    "rawdata_trim = X.to('cpu').numpy()\n",
    "oneDenoise_1 = oneDenoise_1.to('cpu').numpy()\n",
    "mulDenoise_1 = mulDenoise_1.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc17dd",
   "metadata": {},
   "source": [
    "### Denoise many earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1008a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1 = cat[:]\n",
    "inp_data = rawdata[:,:,:]\n",
    "one_denoised = np.zeros_like(inp_data)\n",
    "mul_denoised = np.zeros_like(inp_data)\n",
    "\n",
    "for eid in np.arange(len(inp_data)):\n",
    "    one_denoised[eid,:,:], mul_denoised[eid,:,:] = Denoise_largeDAS(inp_data[eid], \n",
    "                                                                    model_1, \n",
    "                                                                    devc, \n",
    "                                                                    repeat=4, \n",
    "                                                                    norm_batch=False)\n",
    "    \n",
    "# vizRawDenoise(inp_data, one_denoised, mul_denoised, index=range(len(inp_data)), model=\"raw-raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d44887",
   "metadata": {},
   "source": [
    "### Save as H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save denoised data\n",
    "with h5py.File(out_dir + 'das_raw_denoised.hdf5', 'w') as f:\n",
    "    f.create_dataset(\"input_data\", data=inp_data)\n",
    "    f.create_dataset(\"one_denoise\", data=one_denoised)\n",
    "    f.create_dataset(\"mul_denoise\", data=mul_denoised)\n",
    "cat1.write(out_dir + \"denoised_catalog.xml\", format=\"QUAKEML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ae04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizRawDenoise(inp_data, one_denoised, mul_denoised, index=range(10), model=\"raw-raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c309fa",
   "metadata": {},
   "source": [
    "## 3. Pick Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dad68",
   "metadata": {},
   "source": [
    "### Interpolate DAS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b3185",
   "metadata": {},
   "source": [
    "The phase picker needs 6000 time points as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read raw and denoised DAS\n",
    "with h5py.File(out_dir + 'das_raw_denoised.hdf5', 'r') as f:\n",
    "    inp_data = f[\"input_data\"][:]\n",
    "    one_denoised = f[\"one_denoise\"][:]\n",
    "    mul_denoised = f[\"mul_denoise\"][:]\n",
    "\n",
    "cat1 = read_events(out_dir + \"denoised_catalog.xml\")\n",
    "\n",
    "### interpolate\n",
    "interp_func = interp1d(np.linspace(0, 1, 1500), inp_data, axis=-1, kind='linear')\n",
    "interpolated_image = interp_func(np.linspace(0, 1, 6000))\n",
    "interp_func = interp1d(np.linspace(0, 1, 1500), one_denoised, axis=-1, kind='linear')\n",
    "interpolated_onedenoised = interp_func(np.linspace(0, 1, 6000))\n",
    "interp_func = interp1d(np.linspace(0, 1, 1500), mul_denoised, axis=-1, kind='linear')\n",
    "interpolated_muldenoised = interp_func(np.linspace(0, 1, 6000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f5680",
   "metadata": {},
   "source": [
    "### Set up phase picker\n",
    "Emsemble-learning framework for picking (Yuan et al, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ML picker parameters\n",
    "paras_semblance = {'dt':0.01, \n",
    "                   'semblance_order':2, \n",
    "                   'window_flag':True, \n",
    "                   'semblance_win':0.5, \n",
    "                   'weight_flag':'max'}\n",
    "\n",
    "### Download models\n",
    "devcc = try_gpu(i=1)\n",
    "\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "\n",
    "pn_ethz_model.to(devcc)\n",
    "pn_scedc_model.to(devcc)\n",
    "pn_neic_model.to(devcc)\n",
    "pn_geofon_model.to(devcc)\n",
    "pn_stead_model.to(devcc)\n",
    "pn_instance_model.to(devcc)\n",
    "\n",
    "list_models = [pn_ethz_model,pn_scedc_model,pn_neic_model,\n",
    "               pn_geofon_model,pn_stead_model,pn_instance_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118126f3",
   "metadata": {},
   "source": [
    "### Pick many earthquaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ed315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Nevent=len(inp_data)\n",
    "\n",
    "fs=100\n",
    "ch_itv=2  # data are downsampled to pick\n",
    "dchan=9.5714\n",
    "\n",
    "nsta = interpolated_image.shape[1] // ch_itv\n",
    "raw_picks = np.zeros([Nevent, nsta, 2, 2], dtype = np.float32)\n",
    "one_picks = np.zeros([Nevent, nsta, 2, 2], dtype = np.float32)\n",
    "mul_picks = np.zeros([Nevent, nsta, 2, 2], dtype = np.float32)\n",
    "pred_picks = np.zeros([Nevent, nsta, 2], dtype = np.float32)\n",
    "\n",
    "for i in tqdm(np.arange(Nevent)):\n",
    "    \n",
    "    ### Predict arrivals\n",
    "    fiber1, fiber2, _,_,_,_,_ = akdas_tpts(cat1, i, kkfls, terra, terra_early)\n",
    "    array = pd.concat([fiber1.iloc[::-1], fiber2], axis=0)\n",
    "    pred_picks[i, :, 0] = array['tp'].values[::ch_itv]\n",
    "    pred_picks[i, :, 1] = array['ts'].values[::ch_itv]\n",
    "    array_dist = array['dist']/1000\n",
    "    \n",
    "    ### Pick RAW data\n",
    "    image = np.nan_to_num(interpolated_image[i,::ch_itv,:])\n",
    "    raw_picks[i,:,:,:] = apply_elep(image, list_models, fs, paras_semblance, devcc)\n",
    "    \n",
    "    ### Pick oneDENOISED  \n",
    "    image =np.nan_to_num(interpolated_onedenoised[i,::ch_itv,:])\n",
    "    one_picks[i,:,:,:] = apply_elep(image, list_models, fs, paras_semblance, devcc)\n",
    "    \n",
    "    ### Pick mulDENOISED \n",
    "    image = np.nan_to_num(interpolated_muldenoised[i,::ch_itv,:])\n",
    "    mul_picks[i,:,:,:] = apply_elep(image, list_models, fs, paras_semblance, devcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40fbc10",
   "metadata": {},
   "source": [
    "### Save results to H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723356a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(out_dir + 'phase_picks.hdf5', 'w') as f:\n",
    "    f.create_dataset(\"raw_alldata_picks\", data=raw_picks)\n",
    "    f.create_dataset(\"one_denoise_picks\", data=one_picks)\n",
    "    f.create_dataset(\"mul_denoise_picks\", data=mul_picks)\n",
    "    f.create_dataset(\"predicted_picks\", data=pred_picks)\n",
    "    f.create_dataset(\"array_dist\", data=array_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7474b",
   "metadata": {},
   "source": [
    "## 4. Quality Control and Visualization \n",
    "These cells below can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read phase picks from the previous session\n",
    "with h5py.File(out_dir + 'phase_picks_400_555.hdf5', 'r') as f:\n",
    "    raw_picks = f[\"raw_alldata_picks\"][:]\n",
    "    one_picks = f[\"one_denoise_picks\"][:]\n",
    "    mul_picks = f[\"mul_denoise_picks\"][:]\n",
    "    pred_picks = f[\"predicted_picks\"][:]\n",
    "    array_dist = f[\"array_dist\"][:]\n",
    "    \n",
    "### Read raw and denoised DAS\n",
    "with h5py.File(out_dir + 'das_raw_denoised_400_555.hdf5', 'r') as f:\n",
    "    inp_data = f[\"input_data\"][:]\n",
    "    one_denoised = f[\"one_denoise\"][:]\n",
    "    mul_denoised = f[\"mul_denoise\"][:]\n",
    "\n",
    "### Read the catalog\n",
    "cat1 = read_events(out_dir + \"denoised_catalog_400_555.xml\")\n",
    "\n",
    "### Recording time\n",
    "df = pd.read_csv(raw_dir + record_time_file).iloc[200:400]\n",
    "start_times = df['record_time'].values\n",
    "org_times = [evt.origins[0].time - UTCDateTime.strptime(start_t, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') for evt, start_t in zip(cat1 , start_times)]\n",
    "pred_picks += np.array(org_times)[:, np.newaxis, np.newaxis]\n",
    "\n",
    "### interpolate\n",
    "interp_func = interp1d(np.linspace(0, 1, 1500), inp_data, axis=-1, kind='linear')\n",
    "interpolated_image = interp_func(np.linspace(0, 1, 6000))\n",
    "interp_func = interp1d(np.linspace(0, 1, 1500), one_denoised, axis=-1, kind='linear')\n",
    "interpolated_onedenoised = interp_func(np.linspace(0, 1, 6000))\n",
    "interp_func = interp1d(np.linspace(0, 1, 1500), mul_denoised, axis=-1, kind='linear')\n",
    "interpolated_muldenoised = interp_func(np.linspace(0, 1, 6000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ecebf",
   "metadata": {},
   "source": [
    "QC the picks and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce745b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Nevent=len(inp_data)\n",
    "thr=0.05\n",
    "fs=100\n",
    "ch_itv=2  # data are downsampled to pick\n",
    "dchan=9.5714\n",
    "st, ed = 5, 55\n",
    "tax=np.arange(interpolated_image.shape[2])/fs\n",
    "win=np.where(np.logical_and(tax>st,tax<ed))[0]\n",
    "\n",
    "snr_raw = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "snr_one = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "snr_mul = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "npk_raw = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "npk_one = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "npk_mul = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "std_raw = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "std_one = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "std_mul = np.zeros((Nevent, 2), dtype = np.float32)\n",
    "\n",
    "for i in tqdm(np.arange(Nevent)):\n",
    "    evt = cat1[i]\n",
    "    mag = evt.magnitudes[0].mag\n",
    "    lon = evt.origins[0].longitude\n",
    "    lat = evt.origins[0].latitude\n",
    "    dep = evt.origins[0].depth/1000  ## too shallow depth+ long distance = bugs\n",
    "    ort = evt.origins[0].time\n",
    "    \n",
    "    mag0, lon0, lat0, dep0, ort0= \\\n",
    "    round(mag,1), round(lon,2), round(lat,2), round(dep,0), ort.strftime('%Y-%m-%d')\n",
    "\n",
    "    plt.figure(figsize=(24, 8), constrained_layout=True)\n",
    "\n",
    "    ### Raw data\n",
    "    image = np.nan_to_num(interpolated_image[i,::ch_itv,:])\n",
    "    offset_s, std_raw[i,1], mean_offset_s, ind_s = fit_series(\n",
    "        raw_picks[i,:,1,0], pred_picks[i,:,1], raw_picks[i,:,1,1], thr=0.05, vmin=st, vmax=ed)\n",
    "    \n",
    "    offset_p, std_raw[i,0], mean_offset_p, ind_p = fit_series(\n",
    "        raw_picks[i,:,0,0], pred_picks[i,:,0], raw_picks[i,:,0,1], thr=0.1, vmin=st, vmax=ed)\n",
    "    \n",
    "    plt.subplot(2, 6, 1) \n",
    "    subfig_img(image, raw_picks[i], ind_p, ind_s, pred_picks[i], array_dist[::ch_itv],\n",
    "               \"M\"+str(mag0)+\" lat. \"+str(lat0)+\" lon. \"+str(lon0))\n",
    "        \n",
    "    plt.subplot(2, 6, 2) \n",
    "    subfig_histpick(raw_picks[i])\n",
    "    \n",
    "    plt.subplot(2, 6, 7) \n",
    "    snr_raw[i,0], snr_raw[i,1] = subfig_goodtrace(\n",
    "        image, raw_picks[i], ind_p, ind_s, tax, win)\n",
    "    npk_raw[i,0], npk_raw[i,1] = len(ind_p), len(ind_s)\n",
    "            \n",
    "    plt.subplot(2, 6, 8)\n",
    "    plt.hist(offset_p,bins=20,edgecolor='b',fill=False,label='p',range=(-1,1))\n",
    "    plt.hist(offset_s,bins=20,edgecolor='g',fill=False,label='s',range=(-1,1))\n",
    "    plt.title(str(npk_raw[i,0])+\"p|\"+str(npk_raw[i,1])+\"s|\"+\n",
    "              \"SNR \"+str(round(snr_raw[i,0],1))+\"/\"+str(round(snr_raw[i,1],1)))\n",
    "    \n",
    "    ### oneDENOISED  \n",
    "    image = np.nan_to_num(interpolated_onedenoised[i,::ch_itv,:])\n",
    "    offset_s, std_one[i,1], mean_offset_s, ind_s= fit_series(\n",
    "        one_picks[i,:,1,0], pred_picks[i,:,1], one_picks[i,:,1,1], thr=0.05, vmin=st, vmax=ed)\n",
    "    \n",
    "    offset_p, std_one[i,0], mean_offset_p, ind_p= fit_series(\n",
    "        one_picks[i,:,0,0], pred_picks[i,:,0], one_picks[i,:,0,1], thr=0.1, vmin=st, vmax=ed)\n",
    "    \n",
    "    plt.subplot(2, 6, 3) \n",
    "    subfig_img(image, one_picks[i], ind_p, ind_s, pred_picks[i], array_dist[::ch_itv], \n",
    "                \"Ensemble Picking on Denoised data\")\n",
    "    \n",
    "    plt.subplot(2, 6, 4) \n",
    "    subfig_histpick(one_picks[i])\n",
    "    \n",
    "    plt.subplot(2, 6, 9) \n",
    "    snr_one[i,0], snr_one[i,1]=subfig_goodtrace(\n",
    "        image, one_picks[i], ind_p, ind_s, tax, win)\n",
    "    npk_one[i,0], npk_one[i,1] = len(ind_p), len(ind_s)\n",
    "    \n",
    "    plt.subplot(2, 6, 10) \n",
    "    plt.hist(offset_p,bins=20,edgecolor='b',fill=False,label='p',range=(-1,1))\n",
    "    plt.hist(offset_s,bins=20,edgecolor='g',fill=False,label='s',range=(-1,1))\n",
    "    plt.title(str(npk_one[i,0])+\"p|\"+str(npk_one[i,1])+\"s|\"+\n",
    "              \"SNR \"+str(round(snr_one[i,0],1))+\"/\"+str(round(snr_one[i,1],1)))\n",
    "    \n",
    "    ### mulDENOISED \n",
    "    image = np.nan_to_num(interpolated_muldenoised[i,::ch_itv,:])\n",
    "    offset_s, std_mul[i,1], mean_offset_s, ind_s= fit_series(\n",
    "        mul_picks[i,:,1,0], pred_picks[i,:,1], mul_picks[i,:,1,1], thr=0.05, vmin=st, vmax=ed)\n",
    "    \n",
    "    offset_p, std_mul[i,0], mean_offset_p, ind_p= fit_series(\n",
    "        mul_picks[i,:,0,0], pred_picks[i,:,0], mul_picks[i,:,0,1], thr=0.1, vmin=st, vmax=ed)\n",
    "    \n",
    "    plt.subplot(2, 6, 5) \n",
    "    subfig_img(image, mul_picks[i], ind_p, ind_s, pred_picks[i], array_dist[::ch_itv], \n",
    "               \"Ensemble Picking on Denoised data\")\n",
    "    \n",
    "    plt.subplot(2, 6, 6) \n",
    "    subfig_histpick(mul_picks[i])\n",
    "    \n",
    "    plt.subplot(2, 6, 11) \n",
    "    snr_mul[i,0], snr_mul[i,1]= subfig_goodtrace(\n",
    "        image, mul_picks[i], ind_p, ind_s, tax, win)\n",
    "    npk_mul[i,0], npk_mul[i,1]  = len(ind_p), len(ind_s)\n",
    "    \n",
    "    plt.subplot(2, 6, 12) \n",
    "    plt.hist(offset_p,bins=20,edgecolor='b',fill=False,label='p',range=(-1,1))\n",
    "    plt.hist(offset_s,bins=20,edgecolor='g',fill=False,label='s',range=(-1,1))\n",
    "    plt.title(str(npk_mul[i,0])+\"p|\"+str(npk_mul[i,1])+\"s|\"+\n",
    "              \"SNR \"+str(round(snr_mul[i,0],1))+\"/\"+str(round(snr_mul[i,1],1)))\n",
    "\n",
    "            \n",
    "    plt.savefig(f\"{out_dir}picks_event_{i+400}_thr010p005s.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(out_dir + 'pick_stats_400_555_thr010p005s.hdf5', 'w') as f:\n",
    "    f.create_dataset(\"raw_alldata_picks\", data=raw_picks)\n",
    "    f.create_dataset(\"one_denoise_picks\", data=one_picks)\n",
    "    f.create_dataset(\"mul_denoise_picks\", data=mul_picks)\n",
    "    f.create_dataset(\"predicted_picks\", data=pred_picks)\n",
    "    f.create_dataset(\"array_dist\", data=array_dist)\n",
    "    f.create_dataset(\"raw_alldata_snr\", data=snr_raw)\n",
    "    f.create_dataset(\"one_denoise_snr\", data=snr_one)\n",
    "    f.create_dataset(\"mul_denoise_snr\", data=snr_mul)\n",
    "    f.create_dataset(\"raw_alldata_numpick\", data=npk_raw)\n",
    "    f.create_dataset(\"one_denoise_numpick\", data=npk_one)\n",
    "    f.create_dataset(\"mul_denoise_numpick\", data=npk_mul)\n",
    "    f.create_dataset(\"raw_alldata_pickerr\", data=std_raw)\n",
    "    f.create_dataset(\"one_denoise_pickerr\", data=std_one)\n",
    "    f.create_dataset(\"mul_denoise_pickerr\", data=std_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c28c43",
   "metadata": {},
   "source": [
    "## 5. Wiggle anaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read phase picks from the previous session\n",
    "with h5py.File(out_dir + 'phase_picks.hdf5', 'r') as f:\n",
    "    raw_picks = f[\"raw_alldata_picks\"][:]\n",
    "    one_picks = f[\"one_denoise_picks\"][:]\n",
    "    mul_picks = f[\"mul_denoise_picks\"][:]\n",
    "    pred_picks = f[\"predicted_picks\"][:]\n",
    "    array_dist = f[\"array_dist\"][:]\n",
    "    \n",
    "### Read raw and denoised DAS\n",
    "with h5py.File(out_dir + 'das_raw_denoised.hdf5', 'r') as f:\n",
    "    inp_data = f[\"input_data\"][:]\n",
    "    one_denoised = f[\"one_denoise\"][:]\n",
    "    mul_denoised = f[\"mul_denoise\"][:]\n",
    "\n",
    "### Read the catalog\n",
    "cat1 = read_events(out_dir + \"denoised_catalog.xml\")\n",
    "\n",
    "### Recording time\n",
    "df = pd.read_csv(raw_dir + record_time_file)\n",
    "start_times = df['record_time'].values\n",
    "org_times = [evt.origins[0].time - UTCDateTime.strptime(start_t, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') for evt, start_t in zip(cat1 , start_times)]\n",
    "pred_picks += np.array(org_times)[:, np.newaxis, np.newaxis]\n",
    "\n",
    "### interpolate\n",
    "# interp_func = interp1d(np.linspace(0, 1, 1500), inp_data, axis=-1, kind='linear')\n",
    "# interpolated_image = interp_func(np.linspace(0, 1, 6000))\n",
    "# interp_func = interp1d(np.linspace(0, 1, 1500), one_denoised, axis=-1, kind='linear')\n",
    "# interpolated_onedenoised = interp_func(np.linspace(0, 1, 6000))\n",
    "# interp_func = interp1d(np.linspace(0, 1, 1500), mul_denoised, axis=-1, kind='linear')\n",
    "# interpolated_muldenoised = interp_func(np.linspace(0, 1, 6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd616f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_picks.shape, raw_picks.shape, one_picks.shape, mul_picks.shape\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "matplotlib.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0529d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "    \n",
    "maxshift =1\n",
    "cc_thres =0.25\n",
    "######################################################\n",
    "def plot_record(ind, image, picks, fs, tax, t_bef=2, t_aft=3, integrate=False):\n",
    "\n",
    "    wave = np.zeros((len(ind), int((t_bef+t_aft)*fs)), dtype = np.float32)\n",
    "        \n",
    "    for j, ch in enumerate(ind): \n",
    "        pt = int(picks[ch]*fs)\n",
    "        trace = image[ch, pt- t_bef*fs:pt+ t_aft*fs]\n",
    "        if integrate:\n",
    "            int1 = cumulative_trapezoid(trace, tax, initial=0)\n",
    "            wave[j] = (int1-np.mean(int1))\n",
    "        else:\n",
    "            wave[j] = trace * 20\n",
    "\n",
    "    alighed_wave, shifts, cccs = align_channels_twice(wave, ref_ch=0, maxshift=int(maxshift*fs), cc_thres=cc_thres)\n",
    "\n",
    "    if len(alighed_wave)>0:\n",
    "        y = np.arange(alighed_wave.shape[0])\n",
    "        bound = np.percentile(np.fabs(alighed_wave), 80)\n",
    "        plt.pcolormesh(tax,y,alighed_wave, shading='auto',vmin=-bound,vmax=bound,cmap=matplotlib.colormaps['RdBu']) \n",
    "        \n",
    "    stack = np.mean(alighed_wave[cccs>cc_thres,:], axis=0)\n",
    "    \n",
    "    ### plot the stacked trace\n",
    "    plt.plot(tax, stack/np.std(stack)*len(ind)/20, color='g', linestyle='-', lw=2)\n",
    "    plt.xlim(0-t_bef, t_aft)\n",
    "\n",
    "    return shifts, cccs\n",
    "######################################################\n",
    "\n",
    "ch_itv=2  # data are downsampled to pick, so also downsampled to visualize\n",
    "dchan=9.5714\n",
    "fs=25\n",
    "st, ed = 5, 55\n",
    "tax=np.arange(0-2*fs,3*fs)/fs\n",
    "\n",
    "offsets_raw = []\n",
    "offsets_mul = []\n",
    "offsets_mul_noalign = []\n",
    "offsets_raw_noalign = []\n",
    "cccs_raw = []\n",
    "cccs_mul = []\n",
    "\n",
    "### Choose several events from the catalog\n",
    "for i in tqdm(range(0,95)):\n",
    "    evt = cat1[i]\n",
    "    mag = evt.magnitudes[0].mag\n",
    "    lon = evt.origins[0].longitude\n",
    "    lat = evt.origins[0].latitude\n",
    "    dep = evt.origins[0].depth/1000\n",
    "    ort = evt.origins[0].time\n",
    "\n",
    "    mag0, lon0, lat0, dep0, ort0= \\\n",
    "    round(mag,1), round(lon,2), round(lat,2), round(dep,0), ort.strftime('%Y-%m-%d')\n",
    "                  \n",
    "    plt.figure(figsize=(25, 7.5), constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(2, 5, width_ratios=[1, 2, 2, 1, 1])\n",
    "\n",
    "    ## mul denoised\n",
    "    image = np.nan_to_num(mul_denoised[i,::ch_itv,:])\n",
    "\n",
    "    offset_s, std_raw_s, mean_offset_s, ind_s = fit_series(\n",
    "        mul_picks[i,:,1,0], pred_picks[i,:,1], mul_picks[i,:,1,1], thr=0.05, vmin=st, vmax=ed)\n",
    "    \n",
    "    if len(ind_s)==0:\n",
    "        continue\n",
    "\n",
    "    # plt.subplot(gs[1, 0])\n",
    "    # subfig_img(image, mul_picks[i], ind_p, ind_s, pred_picks[i], array_dist[::ch_itv])\n",
    "    # plt.title(\"Denoised\")\n",
    "    # plt.xlabel(\"Record time (s)\")\n",
    "\n",
    "    plt.subplot(gs[1, 1])\n",
    "    shifts1, cccs1 = plot_record(ind_s[ind_s<2250], image, mul_picks[i,:,1,0], fs, tax)\n",
    "    plt.xlabel(\"Time relative to S arrival (s)\")\n",
    "    \n",
    "    plt.subplot(gs[1, 2])\n",
    "    shifts2, cccs2 = plot_record(ind_s[ind_s>=2250], image, mul_picks[i,:,1,0], fs, tax)\n",
    "    plt.xlabel(\"Time relative to S arrival (s)\")\n",
    "\n",
    "    # plt.subplot(gs[1, 3])\n",
    "    cccs = np.concatenate((cccs1, cccs2),axis=None)\n",
    "    # plt.hist(cccs, bins=20,edgecolor='k',fill=False)\n",
    "    # plt.xlim(0,1)\n",
    "    # plt.title('Cross-correlation coefficients')\n",
    "\n",
    "    # plt.subplot(gs[1, 4])\n",
    "    shifts = np.concatenate((shifts1, shifts2),axis=None)/fs\n",
    "    # plt.hist(shifts,bins=20,edgecolor='k',fill=False)\n",
    "    # plt.xlim(-1,1)\n",
    "    # plt.title('Time shifts')\n",
    "\n",
    "    offsets_mul.append(mul_picks[i,ind_s,1,0] + shifts - pred_picks[i,ind_s,1])\n",
    "    offsets_mul_noalign.append(mul_picks[i,ind_s,1,0] - pred_picks[i,ind_s,1])\n",
    "    cccs_mul.append(cccs)\n",
    "\n",
    "    \n",
    "    ### Raw data\n",
    "    image = np.nan_to_num(inp_data[i,::ch_itv,:])\n",
    "\n",
    "    offset_s, std_raw_s, mean_offset_s, ind_s = fit_series(\n",
    "        raw_picks[i,:,1,0], pred_picks[i,:,1], raw_picks[i,:,1,1], thr=0.05, vmin=st, vmax=ed)\n",
    "\n",
    "    # plt.subplot(gs[0, 0]) \n",
    "    # subfig_img(image, raw_picks[i], ind_p, ind_s, pred_picks[i], array_dist[::ch_itv])\n",
    "    # plt.title(\"M\"+str(mag0)+\" lat. \"+str(lat0)+\" lon. \"+str(lon0))\n",
    "            \n",
    "    plt.subplot(gs[0, 1])\n",
    "    shifts1, cccs1 = plot_record(ind_s[ind_s<2250], image, raw_picks[i,:,1,0], fs, tax, integrate=False)\n",
    "    plt.title(\"KKFL-S cable\")\n",
    "    \n",
    "    plt.subplot(gs[0, 2])\n",
    "    shifts2, cccs2 = plot_record(ind_s[ind_s>=2250], image, raw_picks[i,:,1,0], fs, tax, integrate=False)\n",
    "    plt.title(\"TERRA cable\")\n",
    "\n",
    "    # plt.subplot(gs[0, 3])\n",
    "    cccs = np.concatenate((cccs1, cccs2),axis=None)\n",
    "    # plt.hist(cccs, bins=20, edgecolor='k', fill=False)\n",
    "    # plt.xlim(0,1)\n",
    "    # plt.title('Cross-correlation coefficients')\n",
    "\n",
    "    # plt.subplot(gs[0, 4])\n",
    "    shifts = np.concatenate((shifts1, shifts2),axis=None)/fs\n",
    "    # plt.hist(shifts,bins=20,edgecolor='k',fill=False)\n",
    "    # plt.title('Time shifts')\n",
    "    # plt.xlim(-1,1)\n",
    "\n",
    "    offsets_raw.append(raw_picks[i,ind_s,1,0] + shifts - pred_picks[i,ind_s,1])\n",
    "    offsets_raw_noalign.append(raw_picks[i,ind_s,1,0] - pred_picks[i,ind_s,1])\n",
    "    cccs_raw.append(cccs)\n",
    "\n",
    "    plt.savefig(f\"{out_dir}align_event_{i}.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_pick_offsets = np.concatenate(offsets_mul_noalign, axis=None)\n",
    "mul_align_offsets = np.concatenate(offsets_mul, axis=None)\n",
    "cccs_mul = np.concatenate(cccs_mul, axis=None)\n",
    "cccs_raw = np.concatenate(cccs_raw, axis=None)\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(7.5, 7.5), constrained_layout=True)\n",
    "\n",
    "# Plot the S time error histogram\n",
    "plt.hist(mul_pick_offsets, bins=300, alpha=0.5, color='gray', label='Picked S time - prediction')\n",
    "plt.axvline(np.percentile(mul_pick_offsets,2.5), color='gray', linestyle='--', label='95% confidence')\n",
    "plt.axvline(np.percentile(mul_pick_offsets,97.5), color='gray', linestyle='--')\n",
    "\n",
    "plt.hist(mul_align_offsets, bins=300, alpha=0.5, color='b', label='Aligned S time - prediction')\n",
    "plt.axvline(np.percentile(mul_align_offsets,2.5), color='b', linestyle='--', label='95% confidence')\n",
    "plt.axvline(np.percentile(mul_align_offsets,97.5), color='b', linestyle='--')\n",
    "\n",
    "plt.xlabel('Offsets')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "plt.title('S Time Error Histogram')\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(7.5, 7.5), constrained_layout=True)\n",
    "plt.hist(cccs_raw, bins=300, alpha=0.5, color='gray', label='Raw CC')\n",
    "plt.hist(cccs_mul, bins=300, alpha=0.5, color='b', label='Denoised CC')\n",
    "plt.legend()\n",
    "plt.xlabel('CC between traces')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('CC improvement')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8aa27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(mul_pick_offsets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
