{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e105f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/350] train_loss: 12.75088 valid_loss: 0.53913 time per epoch: 3.224 s\n",
      "[  2/350] train_loss: 0.53616 valid_loss: 0.50098 time per epoch: 3.163 s\n",
      "[  3/350] train_loss: 0.50147 valid_loss: 0.50079 time per epoch: 3.171 s\n",
      "[  4/350] train_loss: 0.50408 valid_loss: 0.50148 time per epoch: 3.215 s\n",
      "[  5/350] train_loss: 0.49753 valid_loss: 0.49497 time per epoch: 3.108 s\n",
      "[  6/350] train_loss: 0.50133 valid_loss: 0.50310 time per epoch: 3.133 s\n",
      "[  7/350] train_loss: 0.49751 valid_loss: 0.49605 time per epoch: 3.107 s\n",
      "[  8/350] train_loss: 0.49643 valid_loss: 0.49311 time per epoch: 3.104 s\n",
      "[  9/350] train_loss: 0.50369 valid_loss: 0.49214 time per epoch: 3.072 s\n",
      "[ 10/350] train_loss: 0.49018 valid_loss: 0.49909 time per epoch: 3.092 s\n",
      "[ 11/350] train_loss: 0.49700 valid_loss: 0.49583 time per epoch: 3.073 s\n",
      "[ 12/350] train_loss: 0.49887 valid_loss: 0.49547 time per epoch: 3.058 s\n",
      "[ 13/350] train_loss: 0.48195 valid_loss: 0.49087 time per epoch: 3.056 s\n",
      "[ 14/350] train_loss: 0.48095 valid_loss: 0.48784 time per epoch: 3.040 s\n",
      "[ 15/350] train_loss: 0.48927 valid_loss: 0.48482 time per epoch: 3.049 s\n",
      "[ 16/350] train_loss: 0.48525 valid_loss: 0.48943 time per epoch: 3.050 s\n",
      "[ 17/350] train_loss: 0.47911 valid_loss: 0.47577 time per epoch: 3.036 s\n",
      "[ 18/350] train_loss: 0.48188 valid_loss: 0.48171 time per epoch: 3.048 s\n",
      "[ 19/350] train_loss: 0.47650 valid_loss: 0.47262 time per epoch: 3.067 s\n",
      "[ 20/350] train_loss: 0.46628 valid_loss: 0.48511 time per epoch: 3.055 s\n",
      "[ 21/350] train_loss: 0.47859 valid_loss: 0.47617 time per epoch: 3.043 s\n",
      "[ 22/350] train_loss: 0.46706 valid_loss: 0.47271 time per epoch: 3.051 s\n",
      "[ 23/350] train_loss: 0.46293 valid_loss: 0.45338 time per epoch: 3.070 s\n",
      "[ 24/350] train_loss: 0.46157 valid_loss: 0.46232 time per epoch: 3.082 s\n",
      "[ 25/350] train_loss: 0.45694 valid_loss: 0.47914 time per epoch: 3.052 s\n",
      "[ 26/350] train_loss: 0.48039 valid_loss: 0.46560 time per epoch: 3.048 s\n",
      "[ 27/350] train_loss: 0.48115 valid_loss: 0.47643 time per epoch: 3.041 s\n",
      "[ 28/350] train_loss: 0.46701 valid_loss: 0.46457 time per epoch: 3.032 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m validate_iter \u001b[38;5;241m=\u001b[39m DataLoader(validation_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\" Training \"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m avg_train_losses, \\\n\u001b[0;32m--> 108\u001b[0m avg_valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mvalidate_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m350\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mminimum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DenoDAS/das_denoise_training.py:123\u001b[0m, in \u001b[0;36mtrain_augmentation\u001b[0;34m(train_dataloader, validate_dataloader, model, loss_fn, optimizer, lr_schedule, epochs, patience, device, minimum_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# ======================= training =======================\u001b[39;00m\n\u001b[1;32m    122\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# train mode on\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, ((X, mask), y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    124\u001b[0m     X, mask, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# predict and loss\u001b[39;00m\n",
      "File \u001b[0;32m/home/jupyter_share/anaconda3/envs/seismo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/jupyter_share/anaconda3/envs/seismo/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/jupyter_share/anaconda3/envs/seismo/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/home/jupyter_share/anaconda3/envs/seismo/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mread_one_h5.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     56\u001b[0m tmp \u001b[38;5;241m=\u001b[39m time_data\u001b[38;5;241m.\u001b[39mT[st_ch:(st_ch\u001b[38;5;241m+\u001b[39mNx_sub), :]\n\u001b[1;32m     58\u001b[0m b, a \u001b[38;5;241m=\u001b[39m butter(\u001b[38;5;241m4\u001b[39m, (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m12\u001b[39m), fs\u001b[38;5;241m=\u001b[39msample_rate, btype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbandpass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m filt \u001b[38;5;241m=\u001b[39m \u001b[43mfiltfilt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m sample \u001b[38;5;241m=\u001b[39m filt \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(filt, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m), keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/home/jupyter_share/anaconda3/envs/seismo/lib/python3.8/site-packages/scipy/signal/_signaltools.py:4145\u001b[0m, in \u001b[0;36mfiltfilt\u001b[0;34m(b, a, x, axis, padtype, padlen, method, irlen)\u001b[0m\n\u001b[1;32m   4142\u001b[0m x0 \u001b[38;5;241m=\u001b[39m axis_slice(ext, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   4144\u001b[0m \u001b[38;5;66;03m# Forward filter.\u001b[39;00m\n\u001b[0;32m-> 4145\u001b[0m (y, zf) \u001b[38;5;241m=\u001b[39m \u001b[43mlfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4147\u001b[0m \u001b[38;5;66;03m# Backward filter.\u001b[39;00m\n\u001b[1;32m   4148\u001b[0m \u001b[38;5;66;03m# Create y0 so zi*y0 broadcasts appropriately.\u001b[39;00m\n\u001b[1;32m   4149\u001b[0m y0 \u001b[38;5;241m=\u001b[39m axis_slice(y, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/home/jupyter_share/anaconda3/envs/seismo/lib/python3.8/site-packages/scipy/signal/_signaltools.py:2128\u001b[0m, in \u001b[0;36mlfilter\u001b[0;34m(b, a, x, axis, zi)\u001b[0m\n\u001b[1;32m   2126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sigtools\u001b[38;5;241m.\u001b[39m_linear_filter(b, a, x, axis)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sigtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linear_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import h5py\n",
    "import torch\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from das_util import try_gpu\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from scipy.signal import filtfilt, butter\n",
    "from scipy.signal.windows import tukey\n",
    "from torch.utils.data import DataLoader\n",
    "from das_denoise_models import unet, dataflow, datalabel\n",
    "from das_denoise_training import train_augmentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\" Read a H5 data \"\"\"\n",
    "class read_one_h5(nn.Module):\n",
    "    def __init__(self, h5_directory, Nx_sub=1280, Nt=1280, max_ch=5000, mask_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h5_directory (string): Path to the folder containing all the h5 files.\n",
    "            Nx_sub: number of consecutive rows (DAS channels) to extract as a sub-image\n",
    "            max_ch: upper bound of channels when extracting the sub-image\n",
    "            mask_ratio: percentage of channels to mask\n",
    "            Nt: number of columns (time points) to extract\n",
    "        \"\"\"\n",
    "        self.h5_filepaths = glob.glob(os.path.join(h5_directory, '*.h5'))\n",
    "        self.Nx_sub = Nx_sub\n",
    "        self.Nt = Nt\n",
    "        self.max_ch = max_ch\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h5_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        h5_filepaths = self.h5_filepaths\n",
    "        Nx_sub = self.Nx_sub\n",
    "        Nt = self.Nt\n",
    "        max_ch = self.max_ch\n",
    "        mask_ratio = self.mask_ratio\n",
    "        \n",
    "        # locate the file to read\n",
    "        h5_filepath = self.h5_filepaths[idx]\n",
    "        # read the h5 file\n",
    "        with h5py.File(h5_filepath, 'r') as f:\n",
    "            time_data = f['Acquisition']['Raw[0]']['RawData'][:Nt, 100:(100+max_ch)]\n",
    "        assert time_data.shape[1] == max_ch, \"Not enough #ch to read, reduce max_ch!\"\n",
    "        assert Nx_sub <= max_ch, \"max_ch is smaller than the #ch needed for the sample, reduce Nx_sub!\"\n",
    "        \n",
    "        # slice a random portion as the sample\n",
    "        st_ch = np.random.randint(low=0, high=max_ch - Nx_sub)\n",
    "        tmp = np.zeros((Nx_sub, Nt), dtype=np.float32)\n",
    "        tmp[:Nx_sub,:time_data.shape[0]] = time_data.T[st_ch:(st_ch+Nx_sub), :]\n",
    "        \n",
    "        b, a = butter(4, (0.5, 12), fs=sample_rate, btype='bandpass')\n",
    "        filt = filtfilt(b, a, tmp, axis=-1)\n",
    "        sample = filt / np.std(filt, axis=(0,1), keepdims=True)\n",
    "        sample = sample.astype(np.float32)\n",
    "        \n",
    "        # create a mask for channels: 0 means to mask, 1 means to keep\n",
    "        mask = np.ones((Nx_sub, Nt), dtype=np.float32)\n",
    "        rng = np.random.default_rng()\n",
    "        trace_masked = rng.choice(Nx_sub, size=int(mask_ratio * Nx_sub), replace=False)\n",
    "        mask[trace_masked, :] = mask[trace_masked, :] * 0\n",
    "        \n",
    "        return (sample, mask), sample * (1 - mask)\n",
    "    \n",
    "\n",
    "\"\"\" Model \"\"\"\n",
    "model = unet(1, 16, 1024, factors=(5, 3, 2, 2), use_att=False)\n",
    "devc = try_gpu(i=0)\n",
    "model = nn.DataParallel(model, device_ids=[0,1,2,3])  # comment if gpus<4 \n",
    "model.to(devc)\n",
    "\n",
    "# %% Hyper-parameters for training\n",
    "batch_size = 8\n",
    "lr = 1e-4\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "lr_schedule = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode='min', \n",
    "                                                         factor=0.8, \n",
    "                                                         patience=5, \n",
    "                                                         threshold=0.001, \n",
    "                                                         threshold_mode='rel', \n",
    "                                                         cooldown=0, \n",
    "                                                         min_lr=1e-6, \n",
    "                                                         eps=1e-08, \n",
    "                                                         verbose=True)\n",
    "\n",
    "\"\"\" Data \"\"\"\n",
    "data_terra = '/fd1/QibinShi_data/akdas/qibin_data/kkfln/homer-kkfln'\n",
    "data_kkfln = '/fd1/QibinShi_data/akdas/qibin_data/kkfln/homer-kkfln'\n",
    "### for data on Alaska server\n",
    "# data_terra = '/mnt/qnap/TERRA_FiberA_25Hz'\n",
    "# data_kkfln = '/mnt/qnap/KKFL-S_FIberA_25Hz'\n",
    "sample_rate = 25\n",
    "dchan = 10\n",
    "\n",
    "training_data = read_one_h5(data_terra, Nx_sub=1500, Nt=1500, max_ch=5000, mask_ratio=0.5)\n",
    "validation_data = read_one_h5(data_kkfln, Nx_sub=1500, Nt=1500, max_ch=5000, mask_ratio=0.5)\n",
    "\n",
    "train_iter = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "validate_iter = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "avg_train_losses, \\\n",
    "avg_valid_losses = train_augmentation(train_iter,\n",
    "                                   validate_iter,\n",
    "                                   model,\n",
    "                                   loss_fn,\n",
    "                                   optimizer,\n",
    "                                   lr_schedule=lr_schedule,\n",
    "                                   epochs=350,\n",
    "                                   patience=20,\n",
    "                                   device=devc,\n",
    "                                   minimum_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2fd41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
