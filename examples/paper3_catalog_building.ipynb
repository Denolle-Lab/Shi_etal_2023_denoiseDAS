{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Earthquake Detection at Cook Inlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Qibin Shi\n",
    "\n",
    "Tech support: Yiyu Ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/denoiser/')\n",
    "sys.path.append('../src/ensemble_picker/')\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import h5py\n",
    "import pygmt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pyocto\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance\n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "from datetime import datetime\n",
    "from das_util import try_gpu\n",
    "from detect_util import *\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "import obspy\n",
    "from obspy import UTCDateTime, read_events\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "###\n",
    "filepath = '/fd1/QibinShi_data/akdas/qibin_data/elep_pyocto/coast_only/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Buidling catalog with Only Coastal Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Station availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"IRIS\")\n",
    "t1 = UTCDateTime(\"2023-12-01\")\n",
    "t2 = UTCDateTime(\"2023-12-31\")\n",
    "\n",
    "inventory = client.get_stations(network=\"AK,AV\", channel=\"BH?\",\n",
    "                                starttime=t1, endtime=t2, \n",
    "                                maxlatitude=60.1809, minlatitude=58.5911, \n",
    "                                maxlongitude=-150.6555, minlongitude=-153.7177)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ELEP models\n",
    "devcc = try_gpu(i=1)\n",
    "\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "\n",
    "pn_ethz_model.to(devcc)\n",
    "pn_neic_model.to(devcc)\n",
    "pn_scedc_model.to(devcc)\n",
    "pn_stead_model.to(devcc)\n",
    "pn_geofon_model.to(devcc)\n",
    "pn_instance_model.to(devcc)\n",
    "\n",
    "pn_ethz_model.eval()\n",
    "pn_neic_model.eval()\n",
    "pn_scedc_model.eval()\n",
    "pn_stead_model.eval()\n",
    "pn_geofon_model.eval()\n",
    "pn_instance_model.eval()\n",
    "\n",
    "list_models = [pn_ethz_model,\n",
    "               pn_neic_model,\n",
    "               pn_scedc_model,\n",
    "               pn_stead_model,\n",
    "               pn_geofon_model,\n",
    "               pn_instance_model]\n",
    "\n",
    "### Loop over days\n",
    "for i in range(30):\n",
    "    t1 = UTCDateTime(\"2023-12-07\") + i * 86400\n",
    "    ### loop over stations\n",
    "    for net in inventory:\n",
    "        network = net.code\n",
    "\n",
    "        for sta in net:\n",
    "            station = sta.code\n",
    "            \n",
    "            print(network, station, t1)  \n",
    "            \n",
    "            detect_on_fly(network, station, t1, filepath, 6000, 3000, list_models, devcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare picks for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge picks from all stations\n",
    "csv_list = glob.glob(filepath+'1month/*2023*.csv')\n",
    "all_csv =[]\n",
    "for i in csv_list:\n",
    "    all_csv.append(pd.read_csv(i, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks['time'] = picks['time'].apply(lambda x: (datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%fZ')).timestamp())\n",
    "\n",
    "picks.to_csv(filepath + 'picks_octo.csv', index=False)\n",
    "picks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare stations for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Station table for PyOcto\n",
    "# client = Client(\"IRIS\")\n",
    "# t1 = UTCDateTime(\"2023-12-01\")\n",
    "# t2 = UTCDateTime(\"2023-12-31\")\n",
    "# inventory = client.get_stations(network=\"AK,AV\", channel=\"BH?\",\n",
    "#                                 starttime=t1, endtime=t2, \n",
    "#                                 maxlatitude=60.1809, minlatitude=58.5911, \n",
    "#                                 maxlongitude=-150.6555, minlongitude=-153.7177)\n",
    "# stations_table = pd.DataFrame(columns=['id','longitude','latitude', 'elevation'])\n",
    "\n",
    "# for net in inventory:\n",
    "#     for sta in net:\n",
    "#         station = sta.code\n",
    "#         temp = pd.DataFrame(data={'id': sta.code,\n",
    "#                                   'longitude': sta.longitude,\n",
    "#                                   'latitude': sta.latitude, \n",
    "#                                   'elevation': sta.elevation},index=[0])\n",
    "#         stations_table = pd.concat([stations_table,temp],ignore_index=True)\n",
    "\n",
    "stations_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "\n",
    "velocity_model = pyocto.VelocityModel0D(\n",
    "    p_velocity=7.0,\n",
    "    s_velocity=4.0,\n",
    "    tolerance=2.0,\n",
    ")\n",
    "# associator = pyocto.OctoAssociator.from_area(\n",
    "#     lat=(57, 61),\n",
    "#     lon=(-155, -149),\n",
    "#     zlim=(0, 200),\n",
    "#     time_before=300,\n",
    "#     velocity_model=velocity_model,\n",
    "#     n_picks=6,\n",
    "#     n_p_picks=3,\n",
    "#     n_s_picks=3,\n",
    "#     n_p_and_s_picks=3,\n",
    "# )\n",
    "\n",
    "# associator = pyocto.OctoAssociator.from_area(\n",
    "#     lat=(57, 61),\n",
    "#     lon=(-155, -149),\n",
    "#     zlim=(0, 200),\n",
    "#     time_before=300,\n",
    "#     velocity_model=velocity_model,\n",
    "#     n_picks=10,\n",
    "#     n_p_picks=5,\n",
    "#     n_s_picks=5,\n",
    "#     n_p_and_s_picks=3,\n",
    "# )\n",
    "\n",
    "# associator = pyocto.OctoAssociator.from_area(\n",
    "#     lat=(57, 61),\n",
    "#     lon=(-155, -149),\n",
    "#     zlim=(0, 200),\n",
    "#     time_before=300,\n",
    "#     velocity_model=velocity_model,\n",
    "#     n_picks=20,\n",
    "#     n_p_picks=2,\n",
    "#     n_s_picks=10,\n",
    "#     n_p_and_s_picks=2,\n",
    "# )\n",
    "\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=10,\n",
    "    n_p_picks=3,\n",
    "    n_s_picks=5,\n",
    "    n_p_and_s_picks=3,\n",
    ")\n",
    "    \n",
    "associator.transform_stations(stations_table)\n",
    "# stations_table.to_csv(filepath + 'stations_table.csv', index=False)\n",
    "stations_table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(datetime.fromtimestamp)\n",
    "\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo_staonly.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_staonly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the results\n",
    "events = pd.read_csv(filepath + 'events_detect_octo_staonly.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_staonly.csv')\n",
    "print(len(events), len(all_pick_assignments))\n",
    "\n",
    "### Plot\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", \n",
    "             FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", \n",
    "             MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Catalog with Initial DAS picks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAS cable and initial picks -- save as the format same as stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cable coordinates from Ethan Williams\n",
    "### We only use Ch. 500-5000\n",
    "kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "\n",
    "### picks\n",
    "filepaths = ['/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_0_200.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_200_400.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_400_555.hdf5']\n",
    "\n",
    "raw_alldata_picks = np.concatenate([f[\"raw_alldata_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "mul_denoise_picks = np.concatenate([f[\"mul_denoise_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "\n",
    "### recording times\n",
    "record_time_file = '/fd1/QibinShi_data/akdas/qibin_data/recording_times_smaller.csv'\n",
    "df_record_time = pd.read_csv(record_time_file)\n",
    "b_times = [UTCDateTime.strptime(start_t, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') for start_t in df_record_time['record_time'].values]\n",
    "b_times_terra = [b_terra + 0.88 for b_terra in b_times]\n",
    "b_times_kkfls = [b_terra + 1.20 for b_terra in b_times]\n",
    "\n",
    "### convert and save\n",
    "thr = 0.05\n",
    "ch_dsamp = 10\n",
    "ch_ind = np.arange(0, 4500, ch_dsamp)\n",
    "len_cat = raw_alldata_picks.shape[0]\n",
    "\n",
    "channel_table = pd.DataFrame(columns=['id','longitude','latitude', 'elevation'])\n",
    "\n",
    "for ch in ch_ind:\n",
    "\n",
    "    if ch >= 2250:  # terra\n",
    "        ch1 = int(ch * 2 - 4000) \n",
    "        longitude = terra.loc[ch1, 'lon']\n",
    "        latitude = terra.loc[ch1, 'lat']\n",
    "        elevation = terra.loc[ch1, 'dep']\n",
    "        b_t = b_times_terra\n",
    "    else:  # kkfls\n",
    "        ch1 = 5000 - ch * 2\n",
    "        longitude = kkfls.loc[ch1, 'lon']\n",
    "        latitude = kkfls.loc[ch1, 'lat']\n",
    "        elevation = kkfls.loc[ch1, 'dep']\n",
    "        b_t = b_times_kkfls\n",
    "\n",
    "    temp = pd.DataFrame(data={'id': 'das'+str(ch),\n",
    "                              'longitude': longitude,\n",
    "                              'latitude': latitude, \n",
    "                              'elevation': elevation},index=[0])\n",
    "    channel_table = pd.concat([channel_table,temp],ignore_index=True)\n",
    "\n",
    "    p_raw = [b_t[i] + raw_alldata_picks[i, ch, 0, 0] if raw_alldata_picks[i, ch, 0, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    s_raw = [b_t[i] + raw_alldata_picks[i, ch, 1, 0] if raw_alldata_picks[i, ch, 1, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    p_den = [b_t[i] + mul_denoise_picks[i, ch, 0, 0] if mul_denoise_picks[i, ch, 0, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    s_den = [b_t[i] + mul_denoise_picks[i, ch, 1, 0] if mul_denoise_picks[i, ch, 1, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    \n",
    "    df_raw = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_raw,\n",
    "        'trace_p_arrival': p_raw\n",
    "    })\n",
    "\n",
    "    df_deno = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_den,\n",
    "        'trace_p_arrival': p_den\n",
    "    })\n",
    "\n",
    "    df_raw.to_csv(filepath+'1month/' + 'CIDAS_Ch_' + str(ch) + '_raw' + '.csv')\n",
    "    df_deno.to_csv(filepath+'1month/' + 'CIDAS_Ch_' + str(ch) + '_deno' + '.csv')\n",
    "    channel_table.to_csv(filepath + 'das_channel_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare channel and stations for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_model = pyocto.VelocityModel0D(\n",
    "    p_velocity=7.0,\n",
    "    s_velocity=4.0,\n",
    "    tolerance=0.5,\n",
    ")\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -149),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=10,\n",
    "    n_p_picks=0,\n",
    "    n_s_picks=10,\n",
    "    n_p_and_s_picks=0,\n",
    ")\n",
    "\n",
    "### DAS channels\n",
    "channel_table = pd.read_csv(filepath + 'das_channel_table.csv')[::10]\n",
    "associator.transform_stations(channel_table)\n",
    "\n",
    "### Coastal stations\n",
    "picks_sta = pd.read_csv(filepath + 'picks_octo.csv')\n",
    "sta_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "\n",
    "### Merge\n",
    "stations_table = pd.concat([sta_table, channel_table], axis=0, ignore_index=True)\n",
    "stations_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare picks for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge picks\n",
    "ch_ind = np.arange(0, 4500, 100)\n",
    "all_csv =[]\n",
    "for i in ch_ind:\n",
    "    filename = filepath+'1month/CIDAS_Ch_' + str(i) + '_deno.csv'\n",
    "    all_csv.append(pd.read_csv(filename, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks_das = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks_das['time'] = picks_das['time'].apply(lambda x: (datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%fZ')).timestamp())\n",
    "\n",
    "picks_das.to_csv(filepath + 'picks_octo_das.csv', index=False)\n",
    "\n",
    "### Merge DAS and Stas\n",
    "picks = pd.concat([picks_sta, picks_das], axis=0, ignore_index=True)\n",
    "picks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(datetime.fromtimestamp)\n",
    "\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the results\n",
    "events = pd.read_csv(filepath + 'events_detect_octo.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments.csv')\n",
    "\n",
    "### Plot\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Catalog with Quality-Controlled DAS picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cable coordinates from Ethan Williams\n",
    "### We only use Ch. 500-5000\n",
    "kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "\n",
    "### picks\n",
    "filepaths = ['/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_0_200.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_200_400.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_400_555.hdf5']\n",
    "\n",
    "raw_alldata_picks = np.concatenate([f[\"raw_alldata_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "mul_denoise_picks = np.concatenate([f[\"mul_denoise_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "pred_picks = np.concatenate([f[\"predicted_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "\n",
    "### catalog\n",
    "cat1 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_0_200.xml\")\n",
    "cat2 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_200_400.xml\")\n",
    "cat3 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_400_555.xml\")\n",
    "cat = cat1 + cat2 + cat3\n",
    "\n",
    "### Recording time\n",
    "df_record_time = pd.read_csv('/fd1/QibinShi_data/akdas/qibin_data/recording_times_smaller.csv')\n",
    "b_times = [UTCDateTime.strptime(start_t, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') for start_t in df_record_time['record_time'].values]\n",
    "b_times_terra = [b_terra + 0.88 for b_terra in b_times]\n",
    "b_times_kkfls = [b_terra + 1.20 for b_terra in b_times]\n",
    "org_times = [evt.origins[0].time - b_t for evt, b_t in zip(cat, b_times)]\n",
    "pred_picks += np.array(org_times)[:, np.newaxis, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_series(s1, s2, prob, thr=0.05, vmin=0, vmax=60):\n",
    "    offsets = s1-s2\n",
    "    ind = np.where(np.logical_and(np.logical_and(np.logical_and(vmin<s1, s1<vmax), prob > thr), np.fabs(offsets) < 3.0))[0]\n",
    "    ind_rest = np.setdiff1d(np.arange(len(s1)), ind)\n",
    "\n",
    "    s1[ind_rest] = np.nan\n",
    "\n",
    "    return s1, ind, ind_rest\n",
    "\n",
    "qc_picks = np.zeros_like(mul_denoise_picks)\n",
    "\n",
    "for i in tqdm(np.arange(len(raw_alldata_picks))):\n",
    "    \n",
    "    qc_picks[i, :, 1, 0], ind1, ind2 = fit_series(\n",
    "        mul_denoise_picks[i,:,1,0], pred_picks[i,:,1], raw_alldata_picks[i,:,1,1], thr=0.05, vmin=5, vmax=55)\n",
    "    \n",
    "    qc_picks[i, :, 0, 0], ind3, ind4 = fit_series(\n",
    "        mul_denoise_picks[i,:,0,0], pred_picks[i,:,0], raw_alldata_picks[i,:,0,1], thr=0.1, vmin=5, vmax=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge qc picked with DAS locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_dsamp = 10\n",
    "ch_ind = np.arange(0, 4500, ch_dsamp)\n",
    "len_cat = qc_picks.shape[0]\n",
    "\n",
    "for ch in ch_ind:\n",
    "\n",
    "    if ch >= 2250:  # terra\n",
    "        ch1 = int(ch * 2 - 4000) \n",
    "        longitude = terra.loc[ch1, 'lon']\n",
    "        latitude = terra.loc[ch1, 'lat']\n",
    "        elevation = terra.loc[ch1, 'dep']\n",
    "        b_t = b_times_terra\n",
    "    else:  # kkfls\n",
    "        ch1 = 5000 - ch * 2\n",
    "        longitude = kkfls.loc[ch1, 'lon']\n",
    "        latitude = kkfls.loc[ch1, 'lat']\n",
    "        elevation = kkfls.loc[ch1, 'dep']\n",
    "        b_t = b_times_kkfls\n",
    "\n",
    "    p_den = [b_t[i] + qc_picks[i, ch, 0, 0] if qc_picks[i, ch, 0, 0] == qc_picks[i, ch, 0, 0] else np.nan for i in range(len_cat)]\n",
    "    s_den = [b_t[i] + qc_picks[i, ch, 1, 0] if qc_picks[i, ch, 1, 0] == qc_picks[i, ch, 1, 0] else np.nan for i in range(len_cat)]\n",
    "\n",
    "    df_deno = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_den,\n",
    "        'trace_p_arrival': p_den\n",
    "    })\n",
    "\n",
    "    df_deno.to_csv(filepath+'1month/' + 'CIDAS_Ch_' + str(ch) + '_deno_qc' + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare stations, channel and picks for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Octo\n",
    "velocity_model = pyocto.VelocityModel0D(\n",
    "    p_velocity=7.0,\n",
    "    s_velocity=4.0,\n",
    "    tolerance=2.0,\n",
    ")\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=10,\n",
    "    n_p_picks=3,\n",
    "    n_s_picks=5,\n",
    "    n_p_and_s_picks=3,\n",
    ")\n",
    "\n",
    "### DAS channel table\n",
    "channel_table = pd.read_csv(filepath + 'das_channel_table.csv')[::10]\n",
    "associator.transform_stations(channel_table)\n",
    "\n",
    "### DAS picks\n",
    "ch_ind = np.arange(0, 4500, 100)\n",
    "all_csv =[]\n",
    "for i in ch_ind:\n",
    "    filename = filepath+'1month/CIDAS_Ch_' + str(i) + '_deno_qc.csv'\n",
    "    all_csv.append(pd.read_csv(filename, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks_das = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks_das['time'] = picks_das['time'].apply(lambda x: (datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%fZ')).timestamp())\n",
    "\n",
    "picks_das.to_csv(filepath + 'picks_octo_das_qc.csv', index=False)\n",
    "\n",
    "\n",
    "### Coastal data\n",
    "picks_sta = pd.read_csv(filepath + 'picks_octo.csv')\n",
    "sta_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "\n",
    "### Merge DAS and Sta\n",
    "picks = pd.concat([picks_sta, picks_das], axis=0, ignore_index=True)\n",
    "stations_table = pd.concat([sta_table, channel_table], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(datetime.fromtimestamp)\n",
    "\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo_qc1.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_qc1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the results\n",
    "events = pd.read_csv(filepath + 'events_detect_octo_qc1.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_qc1.csv')\n",
    "\n",
    "### Plot\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
