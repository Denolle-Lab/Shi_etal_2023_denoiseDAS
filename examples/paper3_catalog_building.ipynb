{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earthquake Detection at Cook Inlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Qibin Shi, Tech support: Yiyu Ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/denoiser/')\n",
    "sys.path.append('../src/ensemble_picker/')\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import h5py\n",
    "import pygmt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pyocto\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance\n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "from datetime import datetime\n",
    "from das_util import try_gpu\n",
    "from detect_util import *\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "import obspy\n",
    "from obspy import UTCDateTime, read_events\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "###\n",
    "filepath = '/fd1/QibinShi_data/akdas/qibin_data/elep_pyocto/coast_only/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Catalog Workflow for Coastal Stations Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Station availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"IRIS\")\n",
    "t1 = UTCDateTime(\"2023-12-01\")\n",
    "t2 = UTCDateTime(\"2023-12-31\")\n",
    "\n",
    "inventory = client.get_stations(network=\"AK,AV\", channel=\"BH?\",\n",
    "                                starttime=t1, endtime=t2, \n",
    "                                maxlatitude=60.1809, minlatitude=58.5911, \n",
    "                                maxlongitude=-150.6555, minlongitude=-153.7177)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ELEP models\n",
    "devcc = try_gpu(i=1)\n",
    "\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "\n",
    "pn_ethz_model.to(devcc)\n",
    "pn_neic_model.to(devcc)\n",
    "pn_scedc_model.to(devcc)\n",
    "pn_stead_model.to(devcc)\n",
    "pn_geofon_model.to(devcc)\n",
    "pn_instance_model.to(devcc)\n",
    "\n",
    "pn_ethz_model.eval()\n",
    "pn_neic_model.eval()\n",
    "pn_scedc_model.eval()\n",
    "pn_stead_model.eval()\n",
    "pn_geofon_model.eval()\n",
    "pn_instance_model.eval()\n",
    "\n",
    "list_models = [pn_ethz_model,\n",
    "               pn_neic_model,\n",
    "               pn_scedc_model,\n",
    "               pn_stead_model,\n",
    "               pn_geofon_model,\n",
    "               pn_instance_model]\n",
    "\n",
    "### Loop over days\n",
    "for i in range(30):\n",
    "    t1 = UTCDateTime(\"2023-12-07\") + i * 86400\n",
    "    ### loop over stations\n",
    "    for net in inventory:\n",
    "        network = net.code\n",
    "\n",
    "        for sta in net:\n",
    "            station = sta.code\n",
    "            \n",
    "            print(network, station, t1)  \n",
    "            \n",
    "            detect_on_fly(network, station, t1, filepath, 6000, 3000, list_models, devcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare picks for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge picks from all stations\n",
    "csv_list = glob.glob(filepath+'1month/*2023*.csv')\n",
    "all_csv =[]\n",
    "for i in csv_list:\n",
    "    all_csv.append(pd.read_csv(i, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks['time'] = picks['time'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "\n",
    "picks.to_csv(filepath + 'picks_octo_sta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare stations for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = pd.read_csv(\"../models/IASP91_1.csv\").iloc[:78]\n",
    "pyocto.VelocityModel1D.create_model(layers, 1., 600, 400, \"../models/velocity_model\")\n",
    "\n",
    "### Station table for PyOcto\n",
    "# stations_table = pd.DataFrame(columns=['id','longitude','latitude', 'elevation'])\n",
    "# for net in inventory:\n",
    "#     for sta in net:\n",
    "#         station = sta.code\n",
    "#         temp = pd.DataFrame(data={'id': sta.code,\n",
    "#                                   'longitude': sta.longitude,\n",
    "#                                   'latitude': sta.latitude, \n",
    "#                                   'elevation': sta.elevation},index=[0])\n",
    "#         stations_table = pd.concat([stations_table,temp],ignore_index=True)\n",
    "stations_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "picks = pd.read_csv(filepath + 'picks_octo.csv')\n",
    "\n",
    "velocity_model = pyocto.VelocityModel1D(\"../models/velocity_model\", tolerance=2.0)\n",
    "\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=6,\n",
    "    n_p_picks=3,\n",
    "    n_s_picks=3,\n",
    "    n_p_and_s_picks=2,\n",
    ")\n",
    "    \n",
    "# associator.transform_stations(stations_table)\n",
    "# stations_table.to_csv(filepath + 'stations_table.csv', index=False)\n",
    "stations_table.head(), picks.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(UTCDateTime)\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo_staonly.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_staonly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read results\n",
    "events = pd.read_csv(filepath + 'events_detect_octo_staonly.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_staonly.csv')\n",
    "print(len(events), len(all_pick_assignments))\n",
    "\n",
    "### Plot\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", \n",
    "             FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", \n",
    "             MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Testing DAS (download for USGS catalog events) + Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cable coordinates from Ethan Williams\n",
    "### We only use Ch. 500-5000\n",
    "kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "\n",
    "### picks\n",
    "filepaths = ['/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_0_200.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_200_400.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_400_555.hdf5']\n",
    "\n",
    "raw_alldata_picks = np.concatenate([f[\"raw_alldata_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "mul_denoise_picks = np.concatenate([f[\"mul_denoise_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "pred_picks = np.concatenate([f[\"predicted_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "\n",
    "### catalog\n",
    "cat1 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_0_200.xml\")\n",
    "cat2 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_200_400.xml\")\n",
    "cat3 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_400_555.xml\")\n",
    "cat = cat1 + cat2 + cat3\n",
    "\n",
    "### Recording time\n",
    "df_record_time = pd.read_csv('/fd1/QibinShi_data/akdas/qibin_data/recording_times_smaller.csv')\n",
    "b_times = [UTCDateTime.strptime(start_t, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') for start_t in df_record_time['record_time'].values]\n",
    "b_times_terra = [b_terra + 0.88 for b_terra in b_times]\n",
    "b_times_kkfls = [b_terra + 1.20 for b_terra in b_times]\n",
    "org_times = [evt.origins[0].time - b_t for evt, b_t in zip(cat, b_times)]\n",
    "pred_picks += np.array(org_times)[:, np.newaxis, np.newaxis]\n",
    "\n",
    "### Quality control\n",
    "def fit_series(s1, s2, prob, thr=0.05, vmin=0, vmax=60):\n",
    "    offsets = s1-s2\n",
    "    ind = np.where(np.logical_and(np.logical_and(np.logical_and(vmin<s1, s1<vmax), prob > thr), np.fabs(offsets) < 3.0))[0]\n",
    "    ind_rest = np.setdiff1d(np.arange(len(s1)), ind)\n",
    "    s1[ind_rest] = np.nan\n",
    "\n",
    "    return s1, ind, ind_rest\n",
    "\n",
    "qc_picks = np.zeros_like(mul_denoise_picks)\n",
    "\n",
    "for i in tqdm(np.arange(len(raw_alldata_picks))):  \n",
    "    qc_picks[i, :, 1, 0], ind1, ind2 = fit_series(\n",
    "        mul_denoise_picks[i,:,1,0], pred_picks[i,:,1], raw_alldata_picks[i,:,1,1], thr=0.05, vmin=5, vmax=55)\n",
    "    qc_picks[i, :, 0, 0], ind3, ind4 = fit_series(\n",
    "        mul_denoise_picks[i,:,0,0], pred_picks[i,:,0], raw_alldata_picks[i,:,0,1], thr=0.1, vmin=5, vmax=55)\n",
    "    \n",
    "#### Compose tables for picks and channels\n",
    "ch_dsamp = 10\n",
    "ch_ind = np.arange(0, 4500, ch_dsamp)\n",
    "len_cat = qc_picks.shape[0]\n",
    "\n",
    "channel_table = pd.DataFrame(columns=['id','longitude','latitude', 'elevation'])\n",
    "\n",
    "for ch in ch_ind:\n",
    "\n",
    "    if ch >= 2250:  # terra\n",
    "        ch1 = int(ch * 2 - 4000) \n",
    "        longitude = terra.loc[ch1, 'lon']\n",
    "        latitude = terra.loc[ch1, 'lat']\n",
    "        elevation = terra.loc[ch1, 'dep']\n",
    "        b_t = b_times_terra\n",
    "    else:  # kkfls\n",
    "        ch1 = 5000 - ch * 2\n",
    "        longitude = kkfls.loc[ch1, 'lon']\n",
    "        latitude = kkfls.loc[ch1, 'lat']\n",
    "        elevation = kkfls.loc[ch1, 'dep']\n",
    "        b_t = b_times_kkfls\n",
    "\n",
    "    temp = pd.DataFrame(data={'id': 'das'+str(ch),\n",
    "                              'longitude': longitude,\n",
    "                              'latitude': latitude, \n",
    "                              'elevation': elevation},index=[0])\n",
    "    channel_table = pd.concat([channel_table,temp],ignore_index=True)\n",
    "\n",
    "    p_den = [b_t[i] + qc_picks[i, ch, 0, 0] if qc_picks[i, ch, 0, 0] == qc_picks[i, ch, 0, 0] else np.nan for i in range(len_cat)]\n",
    "    s_den = [b_t[i] + qc_picks[i, ch, 1, 0] if qc_picks[i, ch, 1, 0] == qc_picks[i, ch, 1, 0] else np.nan for i in range(len_cat)]\n",
    "\n",
    "    df_deno = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_den,\n",
    "        'trace_p_arrival': p_den\n",
    "    })\n",
    "\n",
    "    df_deno.to_csv(filepath+'1month/' + 'CIDAS_Ch_' + str(ch) + '_deno_qc' + '.csv')\n",
    "channel_table.to_csv(filepath + 'das_channel_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare stations, channel and picks for association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Octo\n",
    "velocity_model = pyocto.VelocityModel1D(\"../models/velocity_model\", tolerance=2.0)\n",
    "\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=6,\n",
    "    n_p_picks=0,\n",
    "    n_s_picks=5,\n",
    "    n_p_and_s_picks=0,\n",
    ")\n",
    "\n",
    "### DAS channel table\n",
    "channel_table = pd.read_csv(filepath + 'das_channel_table.csv')[::20]\n",
    "associator.transform_stations(channel_table)\n",
    "\n",
    "### DAS picks\n",
    "ch_ind = np.arange(0, 4500, 200)\n",
    "all_csv =[]\n",
    "for i in ch_ind:\n",
    "    filename = filepath+'1month/CIDAS_Ch_' + str(i) + '_deno_qc.csv'\n",
    "    all_csv.append(pd.read_csv(filename, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks_das = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks_das['time'] = picks_das['time'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "\n",
    "picks_das.to_csv(filepath + 'picks_octo_das_qc.csv', index=False)\n",
    "\n",
    "### Coastal data\n",
    "picks_sta = pd.read_csv(filepath + 'picks_octo_sta.csv')\n",
    "sta_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "\n",
    "### Merge DAS and Sta\n",
    "picks = pd.concat([picks_sta, picks_das], axis=0, ignore_index=True)\n",
    "stations_table = pd.concat([sta_table, channel_table], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "print(len(events), len(assignments))\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(UTCDateTime)\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo_qc1.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_qc1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the results\n",
    "events = pd.read_csv(filepath + 'events_detect_octo_qc1.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_qc1.csv')\n",
    "\n",
    "### Plot\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Catalog Workflow for DAS (continuous data) + Coastal Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qibins/anaconda3/envs/denoise/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/qibins/anaconda3/envs/denoise/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src/denoiser/')\n",
    "sys.path.append('../src/ensemble_picker/')\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import h5py\n",
    "import pygmt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyocto\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance\n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "from datetime import datetime\n",
    "from das_util import *\n",
    "from detect_util import *\n",
    "from das_denoise_models import unet\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import filtfilt, butter\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import obspy\n",
    "from obspy import UTCDateTime, read_events\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "###\n",
    "filepath = '/fd1/QibinShi_data/akdas/qibin_data/elep_pyocto/coast_only/'\n",
    "\n",
    "### Initialize Octo\n",
    "velocity_model = pyocto.VelocityModel1D(\"../models/velocity_model\", tolerance=2.0)\n",
    "\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=5,\n",
    "    n_p_picks=0,\n",
    "    n_s_picks=3,\n",
    "    n_p_and_s_picks=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read -- Denoise -- Pick -- Save (Don't run here because we have it on edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Read the data ################################\n",
    "rawdata_dir = '/home/niyiyu/Research/DAS-NIR/datasets/earthquakes_raw/11720793/'\n",
    "kkfls_dir = rawdata_dir + 'KKFL-S/'\n",
    "terra_dir = rawdata_dir + 'TERRA/'\n",
    "kkfls_files = glob.glob(kkfls_dir + 'decimator2_*.h5')\n",
    "terra_files = glob.glob(terra_dir + 'decimator2_*.h5')\n",
    "kkfls_files.sort()\n",
    "terra_files.sort()\n",
    "\n",
    "kkfls_data = np.zeros((len(kkfls_files), 4500, 1500), dtype=np.float32)\n",
    "terra_data = np.zeros((len(terra_files), 4500, 1500), dtype=np.float32)\n",
    "kkfls_btimes = np.zeros(len(kkfls_files), dtype=object)\n",
    "terra_btimes = np.zeros(len(terra_files), dtype=object)\n",
    "\n",
    "for i, kkfls_file in enumerate(kkfls_files):\n",
    "    with h5py.File(kkfls_file, 'r') as f:\n",
    "        time_data = f['Acquisition']['Raw[0]']['RawData'][:1500, 500:5000]\n",
    "        kkfls_data[i, :time_data.shape[1], :time_data.shape[0]] = time_data.T\n",
    "        kkfls_btimes[i] = datetime.utcfromtimestamp(f['Acquisition']['Raw[0]']['RawDataTime'][0]/1e6)\n",
    "for i, terra_file in enumerate(terra_files):\n",
    "    with h5py.File(terra_file, 'r') as f:\n",
    "        time_data = f['Acquisition']['Raw[0]']['RawData'][:1500, 500:5000]\n",
    "        terra_data[i, :time_data.shape[1], :time_data.shape[0]] = time_data.T\n",
    "        terra_btimes[i] = datetime.utcfromtimestamp(f['Acquisition']['Raw[0]']['RawDataTime'][0]/1e6)\n",
    "\n",
    "### merge two arrays and filter\n",
    "rawdata = np.append(kkfls_data[:, ::-1, :], terra_data[:,:,:], axis=1)\n",
    "rawdata = np.nan_to_num(rawdata)\n",
    "\n",
    "### Bandpass filter\n",
    "b, a = butter(4, (0.5, 12), fs=25, btype='bandpass')\n",
    "filt = filtfilt(b, a, rawdata, axis=2)\n",
    "rawdata = filt / np.std(filt, axis=(1,2), keepdims=True)  ## Rawdata w.r.t. Denoised \n",
    "\n",
    "################################ Denoise the data ################################\n",
    "\"\"\" Initialize the U-net model \"\"\"\n",
    "devc = try_gpu(i=0)\n",
    "\n",
    "model_1 = unet(1, 16, 1024, factors=(5, 3, 2, 2), use_att=False)\n",
    "model_1 = nn.DataParallel(model_1, device_ids=[0])\n",
    "model_1.to(devc)\n",
    "\n",
    "\"\"\" Load the pretrained weights \"\"\"\n",
    "model_1.load_state_dict(torch.load('../models/checkpoint_noatt_LRdecays0.8_mask0.5_raw2raw_chmax4500.pt'))  # raw2raw\n",
    "model_1.eval() \n",
    "\n",
    "\"\"\" Denoise \"\"\"\n",
    "one_denoised = np.zeros_like(rawdata)\n",
    "mul_denoised = np.zeros_like(rawdata)\n",
    "\n",
    "for eid in np.arange(len(rawdata)):\n",
    "    one_denoised[eid,:,:], mul_denoised[eid,:,:] = Denoise_largeDAS(rawdata[eid], \n",
    "                                                                    model_1, \n",
    "                                                                    devc, \n",
    "                                                                    repeat=4, \n",
    "                                                                    norm_batch=False)\n",
    "# %matplotlib inline\n",
    "# vizRawDenoise(rawdata[:,::100,:], one_denoised[:,::100,:], mul_denoised[:,::100,:], index=range(21,23), model=\"raw-raw\")\n",
    "\n",
    "################################ Pick phases ################################\n",
    "### Interpolate to 6000 samples\n",
    "interp_func = interp1d(np.linspace(0, 1, 1500), mul_denoised, axis=-1, kind='linear')\n",
    "interpolated_muldenoised = interp_func(np.linspace(0, 1, 6000))\n",
    "\n",
    "### ELEP parameters\n",
    "paras_semblance = {'dt':0.01, \n",
    "                   'semblance_order':2, \n",
    "                   'window_flag':True, \n",
    "                   'semblance_win':0.5, \n",
    "                   'weight_flag':'max'}\n",
    "\n",
    "### ELEP models\n",
    "devcc = try_gpu(i=0)\n",
    "\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "\n",
    "pn_ethz_model.to(devcc); pn_ethz_model.eval()\n",
    "pn_neic_model.to(devcc); pn_neic_model.eval()\n",
    "pn_scedc_model.to(devcc); pn_scedc_model.eval()\n",
    "pn_stead_model.to(devcc); pn_stead_model.eval()\n",
    "pn_geofon_model.to(devcc); pn_geofon_model.eval()\n",
    "pn_instance_model.to(devcc); pn_instance_model.eval()\n",
    "\n",
    "list_models = [pn_ethz_model,\n",
    "               pn_neic_model,\n",
    "               pn_scedc_model,\n",
    "               pn_stead_model,\n",
    "               pn_geofon_model,\n",
    "               pn_instance_model]\n",
    "\n",
    "### Picking\n",
    "fs=100; dchan=9.5714\n",
    "ch_itv=1000  # data are downsampled to pick\n",
    "nsta = interpolated_image.shape[1] // ch_itv\n",
    "mul_picks = np.zeros([len(rawdata), nsta, 2, 2], dtype = np.float32)\n",
    "\n",
    "for i in tqdm(np.arange(len(rawdata))):\n",
    "    ### Pick mulDENOISED \n",
    "    image = np.nan_to_num(interpolated_muldenoised[i,::ch_itv,:])\n",
    "    mul_picks[i,:,:,:] = apply_elep(image, list_models, fs, paras_semblance, devcc)\n",
    "\n",
    "\n",
    "################################ Save phases ################################\n",
    "### Cable coordinates from Ethan Williams\n",
    "### We only use Ch. 500-5000\n",
    "kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "\n",
    "### convert and save\n",
    "thr = 0.05\n",
    "ch_ind = np.arange(raw_picks.shape[1])\n",
    "len_cat = raw_picks.shape[0]\n",
    "\n",
    "for ch in ch_ind:\n",
    "\n",
    "    if ch > 22:  # terra\n",
    "        ch1 = int(ch * 200 - 4000) \n",
    "        longitude = terra.loc[ch1, 'lon']\n",
    "        latitude = terra.loc[ch1, 'lat']\n",
    "        elevation = terra.loc[ch1, 'dep']\n",
    "        b_t = terra_btimes\n",
    "    else:  # kkfls\n",
    "        ch1 = 5000 - ch * 200\n",
    "        longitude = kkfls.loc[ch1, 'lon']\n",
    "        latitude = kkfls.loc[ch1, 'lat']\n",
    "        elevation = kkfls.loc[ch1, 'dep']\n",
    "        b_t = kkfls_btimes\n",
    "\n",
    "    p_den = [b_t[i] + timedelta(seconds=np.float64(mul_picks[i, ch, 0, 0])) if mul_picks[i, ch, 0, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    s_den = [b_t[i] + timedelta(seconds=np.float64(mul_picks[i, ch, 1, 0])) if mul_picks[i, ch, 1, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    \n",
    "    df_deno = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch*100)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_den,\n",
    "        'trace_p_arrival': p_den\n",
    "    })\n",
    "    df_deno.to_csv(filepath+'1hour/' + 'CIDAS_Ch_' + str(ch) + '_deno' + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for association\n",
    "\n",
    "1. QC: removing P picks that are too close to S\n",
    "2. QC: change P to S when S is not picked\n",
    "3. QC: remove picks near the boundaries of the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-f739478a14dd>:19: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
      "<ipython-input-2-f739478a14dd>:20: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
      "<ipython-input-2-f739478a14dd>:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  channel_table = pd.concat([channel_table,temp],ignore_index=True)\n",
      "<ipython-input-2-f739478a14dd>:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ps.dropna(subset=['trace_s_arrival'], how='all', inplace=True)\n",
      "<ipython-input-2-f739478a14dd>:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ps['delay'] = df_ps['trace_s_arrival'].apply(lambda x: UTCDateTime(x).timestamp) - df_ps['trace_p_arrival'].apply(lambda x: UTCDateTime(x).timestamp)\n",
      "<ipython-input-2-f739478a14dd>:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ps.drop(columns=['delay'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "### Initialize Octo\n",
    "velocity_model = pyocto.VelocityModel1D(\"../models/velocity_model\", tolerance=2.0)\n",
    "\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=5,\n",
    "    n_p_picks=0,\n",
    "    n_s_picks=3,\n",
    "    n_p_and_s_picks=0,\n",
    ")\n",
    "\n",
    "### Regenerate channel table for edge cases (because we made mistake for ch numbering on edge)\n",
    "### Cable coordinates from Ethan Williams\n",
    "### We only use Ch. 500-5000\n",
    "kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "ch_itv = 500\n",
    "sel_ch = np.arange(int(ch_itv/2), 9000, ch_itv)\n",
    "nsta = len(sel_ch)\n",
    "channel_table = pd.DataFrame(columns=['id','longitude','latitude', 'elevation'])\n",
    "for ic in np.arange(nsta):\n",
    "\n",
    "    if sel_ch[ic] >= 4500:  # terra\n",
    "        ch1 = int(sel_ch[ic] - 4000) \n",
    "        longitude = terra.loc[ch1, 'lon']\n",
    "        latitude = terra.loc[ch1, 'lat']\n",
    "        elevation = terra.loc[ch1, 'dep']\n",
    "    else:  # kkfls\n",
    "        ch1 = 5000 - sel_ch[ic]\n",
    "        longitude = kkfls.loc[ch1, 'lon']\n",
    "        latitude = kkfls.loc[ch1, 'lat']\n",
    "        elevation = kkfls.loc[ch1, 'dep']\n",
    "\n",
    "    temp = pd.DataFrame(data={'id': 'das'+str(ic*100), ## here is to match the mistake\n",
    "                              'longitude': longitude,\n",
    "                              'latitude': latitude, \n",
    "                              'elevation': elevation},index=[0])\n",
    "    channel_table = pd.concat([channel_table,temp],ignore_index=True)\n",
    "associator.transform_stations(channel_table)\n",
    "channel_table.to_csv(filepath + 'das_channel_table_edge.csv', index=False)\n",
    "\n",
    "### DAS picks QC, remove bad P\n",
    "csv_list = sorted(glob.glob(filepath+'1hour/CIDAS_202312??_??_deno.csv'))\n",
    "all_csv =[]\n",
    "for i in csv_list:\n",
    "    all_csv.append(pd.read_csv(i, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "### P nan, S not nan\n",
    "df_s = df.dropna(subset=['trace_s_arrival'], how='all', inplace=False)\n",
    "df_s = df_s[df_s['trace_p_arrival'].isna()]\n",
    "\n",
    "### P not nan, S nan\n",
    "df_p = df.dropna(subset=['trace_p_arrival'], how='all', inplace=False)\n",
    "df_p = df_p[df_p['trace_s_arrival'].isna()]\n",
    "df_p['trace_s_arrival'] = df_p['trace_p_arrival']\n",
    "df_p['trace_p_arrival'] = np.nan\n",
    "\n",
    "### P S not nan\n",
    "df_ps = df.dropna(subset=['trace_p_arrival'], how='all', inplace=False)\n",
    "df_ps.dropna(subset=['trace_s_arrival'], how='all', inplace=True)\n",
    "df_ps['delay'] = df_ps['trace_s_arrival'].apply(lambda x: UTCDateTime(x).timestamp) - df_ps['trace_p_arrival'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "df_ps.loc[df_ps['delay'] < 0.5, 'trace_p_arrival'] = np.nan\n",
    "df_ps.drop(columns=['delay'], inplace=True)\n",
    "\n",
    "df = pd.concat([df_s, df_p, df_ps], axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks_das = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks_das['sec'] = picks_das['time'].apply(lambda x: int(UTCDateTime(x).strftime('%S')))\n",
    "\n",
    "### QC: removing picks at the boundaries of windows\n",
    "picks_das = picks_das[~picks_das['sec'].between(53, 59)]\n",
    "picks_das = picks_das.drop(columns=['sec'])\n",
    "\n",
    "picks_das['time'] = picks_das['time'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "picks_das.to_csv(filepath + 'picks_octo_das.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read picks and stations\n",
    "picks_sta = pd.read_csv(filepath + 'picks_octo_sta.csv')\n",
    "picks_das = pd.read_csv(filepath + 'picks_octo_das.csv')\n",
    "sta_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "channel_table = pd.read_csv(filepath + 'das_channel_table_edge.csv')\n",
    "picks_sta_subset = picks_sta[picks_sta['time'] < UTCDateTime(2023, 12, 30, 14, 59, 59).timestamp]\n",
    "\n",
    "### Merge\n",
    "stations_table = pd.concat([sta_table, channel_table], axis=0, ignore_index=True)\n",
    "picks = pd.concat([picks_sta_subset, picks_das], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "### Associations\n",
    "\n",
    "### Using DAS+STA\n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(UTCDateTime)\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_das_sta.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_das_sta.csv', index=False)\n",
    "\n",
    "### Using STA only\n",
    "events, assignments = associator.associate(picks_sta_subset, sta_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(UTCDateTime)\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_sta.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_sta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot DAS+STA catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the results\n",
    "events = pd.read_csv(filepath + 'events_detect_das_sta.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_das_sta.csv')\n",
    "\n",
    "### discard events with no sta picks\n",
    "id_to_discard = []\n",
    "for idx in events['idx'].to_numpy():\n",
    "    assign_picks = all_pick_assignments[all_pick_assignments['idx']==idx]\n",
    "    \n",
    "    contains_das = all('das' in station for station in assign_picks['station'])\n",
    "    if contains_das:\n",
    "        id_to_discard.append(idx)\n",
    "\n",
    "events = events[~events['idx'].isin(id_to_discard)]\n",
    "all_pick_assignments = all_pick_assignments[~all_pick_assignments['idx'].isin(id_to_discard)]\n",
    "events.to_csv(filepath + 'events_detect_das_sta_new.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_das_sta_new.csv', index=False)\n",
    "\n",
    "### Plot\n",
    "# grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57, 61])\n",
    "# fig = pygmt.Figure()\n",
    "# pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "#              FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "# pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "# shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "# fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "# pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "# fig.plot(\n",
    "#     x=events[\"longitude\"].values,\n",
    "#     y=events[\"latitude\"].values,\n",
    "#     size=(events[\"picks\"].values + 20) * 0.005,\n",
    "#     fill=events['depth'].values,\n",
    "#     cmap=True,\n",
    "#     style=\"cc\",\n",
    "#     pen=\"black\",\n",
    "# )\n",
    "# fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot STA only catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the results\n",
    "events = pd.read_csv(filepath + 'events_detect_sta.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_sta.csv')\n",
    "events = events[events['depth']>5]\n",
    "\n",
    "### Plot\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Compare catalogs\n",
    "\n",
    "1. DAS+STA vs STA\n",
    "2. DAS+STA vs ANSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAS+STA vs STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events1 = pd.read_csv(filepath + 'events_detect_das_sta_new.csv')\n",
    "events2 = pd.read_csv(filepath + 'events_detect_sta.csv')\n",
    "\n",
    "events1['time'] = events1['time'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "events2['time'] = events2['time'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "\n",
    "matched = []\n",
    "for i in np.arange(len(events1)):\n",
    "    for j in np.arange(len(events2)):\n",
    "        if np.abs(events1.loc[i, 'time'] - events2.loc[j, 'time']) < 10.0:\n",
    "            matched.append([i,j])\n",
    "matched = np.array(matched)\n",
    "\n",
    "matched_idx = np.unique(matched[:,0])\n",
    "unmatched_idx = np.setdiff1d(np.arange(len(events1)), matched_idx)\n",
    "unmatched_events = events1.loc[unmatched_idx]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"01m\", region=[-155, -150, 57, 61])\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=unmatched_events[\"longitude\"].values,\n",
    "    y=unmatched_events[\"latitude\"].values,\n",
    "    size=(unmatched_events[\"picks\"].values) * 0.1,\n",
    "    fill=unmatched_events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAS+STA vs ANSS (USGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anss = pd.read_csv('Dec2023_CookInlet.csv')\n",
    "anss['time'] = anss['time'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "events1 = pd.read_csv(filepath + 'events_detect_das_sta_new.csv')\n",
    "events1['time'] = events1['time'].apply(lambda x: UTCDateTime(x).timestamp)\n",
    "\n",
    "matched = []\n",
    "for i in np.arange(len(events1)):\n",
    "    for j in np.arange(len(anss)):\n",
    "        if np.abs(events1.loc[i, 'time'] - anss.loc[j, 'time']) < 10.0:\n",
    "            matched.append([i,j])\n",
    "matched = np.array(matched)\n",
    "\n",
    "matched_idx = np.unique(matched[:,0])\n",
    "matched_events = events1.loc[matched_idx]\n",
    "unmatched_idx = np.setdiff1d(np.arange(len(events1)), matched_idx)\n",
    "unmatched_events = events1.loc[unmatched_idx]  \n",
    "matched_events.to_csv(filepath + 'matched_events_withANSS.csv', index=False)\n",
    "unmatched_events.to_csv(filepath + 'unmatched_events_withANSS.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>picks</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>578</td>\n",
       "      <td>2023-12-26T20:19:18.818085Z</td>\n",
       "      <td>49.252352</td>\n",
       "      <td>75.289323</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>6</td>\n",
       "      <td>59.672930</td>\n",
       "      <td>-151.625938</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>483</td>\n",
       "      <td>2023-12-23T11:54:51.352410Z</td>\n",
       "      <td>26.703083</td>\n",
       "      <td>92.795609</td>\n",
       "      <td>71.484375</td>\n",
       "      <td>5</td>\n",
       "      <td>59.832116</td>\n",
       "      <td>-152.023858</td>\n",
       "      <td>71.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>451</td>\n",
       "      <td>2023-12-21T14:21:16.010517Z</td>\n",
       "      <td>10.087831</td>\n",
       "      <td>61.284294</td>\n",
       "      <td>46.484375</td>\n",
       "      <td>12</td>\n",
       "      <td>59.550007</td>\n",
       "      <td>-152.321631</td>\n",
       "      <td>46.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>425</td>\n",
       "      <td>2023-12-20T14:03:29.801591Z</td>\n",
       "      <td>37.384316</td>\n",
       "      <td>84.042466</td>\n",
       "      <td>66.015625</td>\n",
       "      <td>6</td>\n",
       "      <td>59.752727</td>\n",
       "      <td>-151.834979</td>\n",
       "      <td>66.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>411</td>\n",
       "      <td>2023-12-20T06:09:40.256129Z</td>\n",
       "      <td>32.637101</td>\n",
       "      <td>56.907722</td>\n",
       "      <td>12.109375</td>\n",
       "      <td>9</td>\n",
       "      <td>59.509574</td>\n",
       "      <td>-151.923610</td>\n",
       "      <td>12.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>377</td>\n",
       "      <td>2023-12-18T21:55:20.870663Z</td>\n",
       "      <td>27.889886</td>\n",
       "      <td>61.284294</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>5</td>\n",
       "      <td>59.549200</td>\n",
       "      <td>-152.006872</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>339</td>\n",
       "      <td>2023-12-16T15:11:02.110581Z</td>\n",
       "      <td>18.395457</td>\n",
       "      <td>68.286808</td>\n",
       "      <td>41.796875</td>\n",
       "      <td>5</td>\n",
       "      <td>59.612580</td>\n",
       "      <td>-152.174134</td>\n",
       "      <td>41.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>330</td>\n",
       "      <td>2023-12-16T04:29:11.319999Z</td>\n",
       "      <td>38.571119</td>\n",
       "      <td>89.294351</td>\n",
       "      <td>82.421875</td>\n",
       "      <td>13</td>\n",
       "      <td>59.799756</td>\n",
       "      <td>-151.812901</td>\n",
       "      <td>82.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>319</td>\n",
       "      <td>2023-12-15T19:36:05.891870Z</td>\n",
       "      <td>27.889886</td>\n",
       "      <td>72.663380</td>\n",
       "      <td>25.390625</td>\n",
       "      <td>5</td>\n",
       "      <td>59.651338</td>\n",
       "      <td>-152.005374</td>\n",
       "      <td>25.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>236</td>\n",
       "      <td>2023-12-12T09:17:11.016847Z</td>\n",
       "      <td>46.878745</td>\n",
       "      <td>104.174695</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>7</td>\n",
       "      <td>59.932450</td>\n",
       "      <td>-151.661573</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>217</td>\n",
       "      <td>2023-12-11T16:38:41.993750Z</td>\n",
       "      <td>30.263494</td>\n",
       "      <td>56.907722</td>\n",
       "      <td>12.890625</td>\n",
       "      <td>5</td>\n",
       "      <td>59.509752</td>\n",
       "      <td>-151.965527</td>\n",
       "      <td>12.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>166</td>\n",
       "      <td>2023-12-09T06:46:12.982939Z</td>\n",
       "      <td>24.329475</td>\n",
       "      <td>68.286808</td>\n",
       "      <td>55.078125</td>\n",
       "      <td>14</td>\n",
       "      <td>59.612277</td>\n",
       "      <td>-152.069019</td>\n",
       "      <td>55.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>38</td>\n",
       "      <td>2023-12-02T20:46:17.072360Z</td>\n",
       "      <td>46.878745</td>\n",
       "      <td>96.296866</td>\n",
       "      <td>46.484375</td>\n",
       "      <td>11</td>\n",
       "      <td>59.861747</td>\n",
       "      <td>-151.663353</td>\n",
       "      <td>46.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>2023-12-02T14:57:24.549478Z</td>\n",
       "      <td>30.263494</td>\n",
       "      <td>56.907722</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>5</td>\n",
       "      <td>59.509752</td>\n",
       "      <td>-151.965527</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-12-01T05:51:05.849096Z</td>\n",
       "      <td>13.648242</td>\n",
       "      <td>96.296866</td>\n",
       "      <td>3.515625</td>\n",
       "      <td>25</td>\n",
       "      <td>59.864179</td>\n",
       "      <td>-152.256405</td>\n",
       "      <td>3.515625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                         time          x           y          z  \\\n",
       "570  578  2023-12-26T20:19:18.818085Z  49.252352   75.289323   0.390625   \n",
       "475  483  2023-12-23T11:54:51.352410Z  26.703083   92.795609  71.484375   \n",
       "443  451  2023-12-21T14:21:16.010517Z  10.087831   61.284294  46.484375   \n",
       "417  425  2023-12-20T14:03:29.801591Z  37.384316   84.042466  66.015625   \n",
       "403  411  2023-12-20T06:09:40.256129Z  32.637101   56.907722  12.109375   \n",
       "369  377  2023-12-18T21:55:20.870663Z  27.889886   61.284294   0.390625   \n",
       "331  339  2023-12-16T15:11:02.110581Z  18.395457   68.286808  41.796875   \n",
       "322  330  2023-12-16T04:29:11.319999Z  38.571119   89.294351  82.421875   \n",
       "312  319  2023-12-15T19:36:05.891870Z  27.889886   72.663380  25.390625   \n",
       "230  236  2023-12-12T09:17:11.016847Z  46.878745  104.174695   0.390625   \n",
       "211  217  2023-12-11T16:38:41.993750Z  30.263494   56.907722  12.890625   \n",
       "162  166  2023-12-09T06:46:12.982939Z  24.329475   68.286808  55.078125   \n",
       "35    38  2023-12-02T20:46:17.072360Z  46.878745   96.296866  46.484375   \n",
       "32    34  2023-12-02T14:57:24.549478Z  30.263494   56.907722   0.390625   \n",
       "4      4  2023-12-01T05:51:05.849096Z  13.648242   96.296866   3.515625   \n",
       "\n",
       "     picks   latitude   longitude      depth  \n",
       "570      6  59.672930 -151.625938   0.390625  \n",
       "475      5  59.832116 -152.023858  71.484375  \n",
       "443     12  59.550007 -152.321631  46.484375  \n",
       "417      6  59.752727 -151.834979  66.015625  \n",
       "403      9  59.509574 -151.923610  12.109375  \n",
       "369      5  59.549200 -152.006872   0.390625  \n",
       "331      5  59.612580 -152.174134  41.796875  \n",
       "322     13  59.799756 -151.812901  82.421875  \n",
       "312      5  59.651338 -152.005374  25.390625  \n",
       "230      7  59.932450 -151.661573   0.390625  \n",
       "211      5  59.509752 -151.965527  12.890625  \n",
       "162     14  59.612277 -152.069019  55.078125  \n",
       "35      11  59.861747 -151.663353  46.484375  \n",
       "32       5  59.509752 -151.965527   0.390625  \n",
       "4       25  59.864179 -152.256405   3.515625  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_events = unmatched_events[unmatched_events['depth']<120]\n",
    "check_events = check_events[check_events['latitude'] < 60.0]\n",
    "check_events = check_events[check_events['latitude'] > 59.5]\n",
    "check_events = check_events[check_events['longitude'] < -151.5]\n",
    "check_events = check_events[check_events['longitude'] > -152.5]\n",
    "check_events['time'] = check_events['time'].apply(lambda x: UTCDateTime(x))\n",
    "check_events.sort_values(by='idx', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Stats to show the significance of DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b555ce029139>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  assignments['time_pick'] = assignments['time_pick'].apply(lambda x: UTCDateTime(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.23076923076923078, 0.23052144151399234)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_das_sta_new.csv')\n",
    "assignments = all_pick_assignments[all_pick_assignments['phase']=='S']\n",
    "\n",
    "assignments['time_pick'] = assignments['time_pick'].apply(lambda x: UTCDateTime(x))\n",
    "assignments_unmatched = assignments[assignments['idx'].isin(unmatched_idx)]\n",
    "das_rows = assignments[assignments['station'].str.contains('das')]\n",
    "len(assignments_unmatched[assignments_unmatched['station'].str.contains('das')])/len(assignments_unmatched), len(das_rows)/len(assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pick_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41411042944785276"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### How many events are seen by DAS\n",
    "id_das_event = []\n",
    "for idx in events1['idx'].to_numpy():\n",
    "    assign_picks = all_pick_assignments[all_pick_assignments['idx']==idx]\n",
    "\n",
    "    contains_das = any('das' in station for station in assign_picks['station'])\n",
    "    if contains_das:\n",
    "        id_das_event.append(idx)\n",
    "\n",
    "len(id_das_event)/len(events1['idx'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>time_pick</th>\n",
       "      <th>time</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>BRLK</td>\n",
       "      <td>2023-12-26T20:19:14.208393Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>BRLK</td>\n",
       "      <td>2023-12-26T20:19:21.628393Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>BRSE</td>\n",
       "      <td>2023-12-26T20:19:14.528393Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>BRSE</td>\n",
       "      <td>2023-12-26T20:19:22.528393Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>CNP</td>\n",
       "      <td>2023-12-26T20:19:15.808397Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>HOM</td>\n",
       "      <td>2023-12-26T20:19:15.568394Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8001</th>\n",
       "      <td>HOM</td>\n",
       "      <td>2023-12-26T20:19:23.808394Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8002</th>\n",
       "      <td>ILCB</td>\n",
       "      <td>2023-12-26T20:19:21.460000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8003</th>\n",
       "      <td>ILCB</td>\n",
       "      <td>2023-12-26T20:19:34.600000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8004</th>\n",
       "      <td>ILNE</td>\n",
       "      <td>2023-12-26T20:19:21.040000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8005</th>\n",
       "      <td>ILNE</td>\n",
       "      <td>2023-12-26T20:19:33.660000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8006</th>\n",
       "      <td>ILS</td>\n",
       "      <td>2023-12-26T20:19:21.280000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>ILS</td>\n",
       "      <td>2023-12-26T20:19:34.060000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8008</th>\n",
       "      <td>ILSW</td>\n",
       "      <td>2023-12-26T20:19:21.628000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8009</th>\n",
       "      <td>ILSW</td>\n",
       "      <td>2023-12-26T20:19:34.528000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8010</th>\n",
       "      <td>IVE</td>\n",
       "      <td>2023-12-26T20:19:20.820000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>IVE</td>\n",
       "      <td>2023-12-26T20:19:33.260000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>das0</td>\n",
       "      <td>2023-12-26T20:19:30.770000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>das100</td>\n",
       "      <td>2023-12-26T20:19:29.530002Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>das1000</td>\n",
       "      <td>2023-12-26T20:19:24.530000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>das1200</td>\n",
       "      <td>2023-12-26T20:19:25.880000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8016</th>\n",
       "      <td>das1300</td>\n",
       "      <td>2023-12-26T20:19:26.349999Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>das200</td>\n",
       "      <td>2023-12-26T20:19:28.509999Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>das500</td>\n",
       "      <td>2023-12-26T20:19:26.220000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>das600</td>\n",
       "      <td>2023-12-26T20:19:25.310001Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8020</th>\n",
       "      <td>das700</td>\n",
       "      <td>2023-12-26T20:19:24.520000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>das800</td>\n",
       "      <td>2023-12-26T20:19:23.940000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8022</th>\n",
       "      <td>das900</td>\n",
       "      <td>2023-12-26T20:19:23.870000Z</td>\n",
       "      <td>2023-12-26T20:19:03.476723Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      station                    time_pick                         time phase\n",
       "7995     BRLK  2023-12-26T20:19:14.208393Z  2023-12-26T20:19:03.476723Z     P\n",
       "7996     BRLK  2023-12-26T20:19:21.628393Z  2023-12-26T20:19:03.476723Z     S\n",
       "7997     BRSE  2023-12-26T20:19:14.528393Z  2023-12-26T20:19:03.476723Z     P\n",
       "7998     BRSE  2023-12-26T20:19:22.528393Z  2023-12-26T20:19:03.476723Z     S\n",
       "7999      CNP  2023-12-26T20:19:15.808397Z  2023-12-26T20:19:03.476723Z     P\n",
       "8000      HOM  2023-12-26T20:19:15.568394Z  2023-12-26T20:19:03.476723Z     P\n",
       "8001      HOM  2023-12-26T20:19:23.808394Z  2023-12-26T20:19:03.476723Z     S\n",
       "8002     ILCB  2023-12-26T20:19:21.460000Z  2023-12-26T20:19:03.476723Z     P\n",
       "8003     ILCB  2023-12-26T20:19:34.600000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8004     ILNE  2023-12-26T20:19:21.040000Z  2023-12-26T20:19:03.476723Z     P\n",
       "8005     ILNE  2023-12-26T20:19:33.660000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8006      ILS  2023-12-26T20:19:21.280000Z  2023-12-26T20:19:03.476723Z     P\n",
       "8007      ILS  2023-12-26T20:19:34.060000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8008     ILSW  2023-12-26T20:19:21.628000Z  2023-12-26T20:19:03.476723Z     P\n",
       "8009     ILSW  2023-12-26T20:19:34.528000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8010      IVE  2023-12-26T20:19:20.820000Z  2023-12-26T20:19:03.476723Z     P\n",
       "8011      IVE  2023-12-26T20:19:33.260000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8012     das0  2023-12-26T20:19:30.770000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8013   das100  2023-12-26T20:19:29.530002Z  2023-12-26T20:19:03.476723Z     S\n",
       "8014  das1000  2023-12-26T20:19:24.530000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8015  das1200  2023-12-26T20:19:25.880000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8016  das1300  2023-12-26T20:19:26.349999Z  2023-12-26T20:19:03.476723Z     S\n",
       "8017   das200  2023-12-26T20:19:28.509999Z  2023-12-26T20:19:03.476723Z     S\n",
       "8018   das500  2023-12-26T20:19:26.220000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8019   das600  2023-12-26T20:19:25.310001Z  2023-12-26T20:19:03.476723Z     S\n",
       "8020   das700  2023-12-26T20:19:24.520000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8021   das800  2023-12-26T20:19:23.940000Z  2023-12-26T20:19:03.476723Z     S\n",
       "8022   das900  2023-12-26T20:19:23.870000Z  2023-12-26T20:19:03.476723Z     S"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pick_assignments['time_pick'] = all_pick_assignments['time_pick'].apply(lambda x: UTCDateTime(x))\n",
    "all_pick_assignments.loc[all_pick_assignments['idx']==577, ['station','time_pick','time','phase']]\n",
    "\n",
    "##  451, 166, 38\n",
    "##\n",
    "# 2023-12-21T14:21:30\n",
    "# 2023-12-09T06:46:27\n",
    "# 2023-12-02T20:46:47\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 441)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmatched_events), len(events1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
