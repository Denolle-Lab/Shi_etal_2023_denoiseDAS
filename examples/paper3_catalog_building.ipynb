{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Earthquake Detection at Cook Inlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Qibin Shi\n",
    "\n",
    "Tech support: Yiyu Ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/denoiser/')\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pygmt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance\n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from das_util import try_gpu\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import obspy\n",
    "from obspy import UTCDateTime, read_events\n",
    "from obspy.clients.fdsn.client import Client\n",
    "\n",
    "import pyocto\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "matplotlib.rcParams['font.size'] = 22\n",
    "\n",
    "filepath = '/fd1/QibinShi_data/akdas/qibin_data/elep_pyocto/coast_only/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Availability of Coastal Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"IRIS\")\n",
    "t1 = UTCDateTime(\"2023-12-01\")\n",
    "t2 = UTCDateTime(\"2023-12-31\")\n",
    "\n",
    "inventory = client.get_stations(network=\"AK,AV\", channel=\"BH?\",\n",
    "                                starttime=t1, endtime=t2, \n",
    "                                maxlatitude=60.1809, minlatitude=58.5911, \n",
    "                                maxlongitude=-150.6555, minlongitude=-153.7177)\n",
    "\n",
    "inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect Earthquakes on Stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to pick many windows\n",
    "def apply_elep(data, list_models, paras_semblance, device):\n",
    "    \"\"\"\n",
    "    Input: data of 1 station, 3 components, many time windows\n",
    "           several eqT models (already to device and eval mode)\n",
    "    Output: semblance for P or S\n",
    "    \"\"\"\n",
    "\n",
    "    ### Normalize data\n",
    "    tmp = data - np.mean(data, axis=-1, keepdims= True)\n",
    "    mmax = np.max(np.abs(tmp), axis=-1, keepdims=True)\n",
    "    data_max = np.divide(tmp , mmax, out=np.zeros_like(tmp), where=(mmax!=0))\n",
    "    data_tt = torch.from_numpy(data_max).to(device, dtype=torch.float32)\n",
    "    \n",
    "    ### ELEP workflow\n",
    "    twin = 6000  ## constant for EQTransformer\n",
    "    nwin = data.shape[0]\n",
    "   \n",
    "    # predictions from all models\n",
    "    batch_pred_P = np.zeros((len(list_models),nwin,twin)) \n",
    "    batch_pred_S = np.zeros((len(list_models),nwin,twin))\n",
    "    smb = np.zeros((nwin,2,twin), dtype = np.float32)\n",
    "    \n",
    "    for ii, imodel in enumerate(list_models):\n",
    "        with torch.no_grad():\n",
    "            batch_pred_P[ii, :, :] = imodel(data_tt)[1].cpu().numpy()[:, :]\n",
    "            batch_pred_S[ii, :, :] = imodel(data_tt)[2].cpu().numpy()[:, :]\n",
    "    \n",
    "    # semblance of all predictions\n",
    "    smb[:,0,:] =np.array(Parallel(n_jobs=100)(delayed(ensemble_semblance)(batch_pred_P[:, iwin, :], paras_semblance) \n",
    "                                                    for iwin in range(nwin)))\n",
    "    smb[:,1,:] =np.array(Parallel(n_jobs=100)(delayed(ensemble_semblance)(batch_pred_S[:, iwin, :], paras_semblance) \n",
    "                                                    for iwin in range(nwin)))\n",
    "    \n",
    "    del batch_pred_P, batch_pred_S, data_tt, data_max, mmax, tmp\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return smb\n",
    "\n",
    "\n",
    "def detect_on_fly(network, station, t1, filepath, width, stride, list_models, devcc):\n",
    "    '''\n",
    "    The workflow: \n",
    "    1) download data for 1 station\n",
    "    2) window the data with stride\n",
    "    3) apply ELEP\n",
    "    4) save picks\n",
    "    '''\n",
    "   ########################\n",
    "   ### Download waveforms\n",
    "    try:\n",
    "        sdata = client.get_waveforms(network=network, \n",
    "                                     station=station,\n",
    "                                     location=\"*\", \n",
    "                                     channel=\"BH?\", \n",
    "                                     starttime=t1,\n",
    "                                     endtime=t1 + 86400)\n",
    "        \n",
    "    except obspy.clients.fdsn.header.FDSNNoDataException:\n",
    "        print(f\"--- No data for {network}.{station} on {t1} ---\")\n",
    "        return\n",
    "    \n",
    "    fs_all = [tr.stats.sampling_rate for tr in sdata]\n",
    "    fs = np.round(fs_all[0])\n",
    "    if len(np.unique(np.array(fs_all))) > 1:      \n",
    "        print(f\"--- Sampling rates are different for {network}.{station} on {t1} ---\")\n",
    "        sdata = sdata.resample(fs)\n",
    "    \n",
    "    sdata.merge(fill_value='interpolate')  # fill gaps\n",
    "    sdata.filter(type='bandpass',freqmin=0.5,freqmax=12)\n",
    "    btime = sdata[0].stats.starttime\n",
    "    \n",
    "\n",
    "    # align 3 components\n",
    "    max_b = max([tr.stats.starttime for tr in sdata])\n",
    "    min_e = min([tr.stats.endtime for tr in sdata])\n",
    "    for tr in sdata:\n",
    "        tr.trim(starttime=max_b, endtime=min_e, nearest_sample=True)    \n",
    "        \n",
    "    ########################\n",
    "    ### Window data\n",
    "    arr_sdata = np.array(sdata)\n",
    "    if len(arr_sdata.shape) == 1:\n",
    "        arr_sdata = arr_sdata[np.newaxis, :] # add dimension\n",
    "    if arr_sdata.shape[0] == 1:\n",
    "        arr_sdata = np.repeat(arr_sdata, 3, axis=0)\n",
    "    elif arr_sdata.shape[0] == 2:\n",
    "        arr_sdata = np.vstack((arr_sdata, arr_sdata[1]))\n",
    "    elif arr_sdata.shape[0] > 3:\n",
    "        arr_sdata = arr_sdata[:3]\n",
    "\n",
    "    nwin = (arr_sdata.shape[1] - width) // stride\n",
    "    if nwin < 1: \n",
    "        print(f\"--- Data too short for {network}.{station} on {t1} ---\")\n",
    "        return\n",
    "    arr_sdata = arr_sdata[:, :int(nwin * stride + width)]\n",
    "    \n",
    "    win_idx = np.zeros(nwin, dtype=np.int32)\n",
    "    windows = np.zeros((nwin, 3, width), dtype= np.float32)\n",
    "\n",
    "    for iwin in range(nwin):\n",
    "        idx = iwin * stride\n",
    "        win_idx[iwin] = idx\n",
    "        windows[iwin,:,:] = arr_sdata[:, idx:idx+width]\n",
    "        \n",
    "    ########################\n",
    "    ### Apply ELEP\n",
    "    paras_semblance = {'dt':1/fs, \n",
    "                       'semblance_order':2, \n",
    "                       'window_flag':True, \n",
    "                       'semblance_win':0.5, \n",
    "                       'weight_flag':'max'}\n",
    "    \n",
    "    smb = apply_elep(windows, list_models, paras_semblance, devcc)\n",
    "    smb_all = np.zeros_like(arr_sdata[0:2])\n",
    "\n",
    "    for iwin in range(nwin):\n",
    "        idx = iwin * stride\n",
    "        smb_all[0, idx+stride:idx+width] = smb[iwin,0,stride:]\n",
    "        smb_all[1, idx+stride:idx+width] = smb[iwin,1,stride:]\n",
    "\n",
    "    p_picks = picks_summary_simple(smb_all[0], 0.10)\n",
    "    s_picks = picks_summary_simple(smb_all[1], 0.05)\n",
    "\n",
    "    ########################\n",
    "    ### Save picks\n",
    "    len_picks = len(p_picks + s_picks)\n",
    "    df = pd.DataFrame({\n",
    "        'event_id': [' '] * len_picks,\n",
    "        'source_type': [' '] * len_picks,\n",
    "        'station_network_code': [network] * len_picks,\n",
    "        'station_channel_code': [' '] * len_picks,\n",
    "        'station_code': [station] * len_picks,\n",
    "        'station_location_code': [sdata[0].stats.location] * len_picks,\n",
    "        'station_latitude_deg': [inventory[0][0].latitude] * len_picks,\n",
    "        'station_longitude_deg': [inventory[0][0].longitude] * len_picks,\n",
    "        'station_elevation_m': [inventory[0][0].elevation] * len_picks,\n",
    "        'trace_name': [' '] * len_picks,\n",
    "        'trace_sampling_rate_hz': [sdata[0].stats.sampling_rate] * len_picks,\n",
    "        'trace_start_time': [sdata[0].stats.starttime] * len_picks,\n",
    "        'trace_S_arrival_sample': [' '] * len_picks,\n",
    "        'trace_P_arrival_sample': [' '] * len_picks,\n",
    "        'trace_S_onset': [' '] * len_picks,\n",
    "        'trace_P_onset': [' '] * len_picks,\n",
    "        'trace_snr_db': [' '] * len_picks,\n",
    "        'trace_s_arrival': [np.nan] * len(p_picks) + [str(btime + idx / fs) for idx in s_picks],\n",
    "        'trace_p_arrival': [str(btime + idx / fs) for idx in p_picks] + [np.nan] * len(s_picks)\n",
    "    })\n",
    "\n",
    "    df.to_csv(filepath+'1month/' + network + '_' + station + '_' + t1.strftime('%Y%m%d') + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ELEP models\n",
    "devcc = try_gpu(i=1)\n",
    "\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "\n",
    "pn_ethz_model.to(devcc)\n",
    "pn_neic_model.to(devcc)\n",
    "pn_scedc_model.to(devcc)\n",
    "pn_stead_model.to(devcc)\n",
    "pn_geofon_model.to(devcc)\n",
    "pn_instance_model.to(devcc)\n",
    "\n",
    "pn_ethz_model.eval()\n",
    "pn_neic_model.eval()\n",
    "pn_scedc_model.eval()\n",
    "pn_stead_model.eval()\n",
    "pn_geofon_model.eval()\n",
    "pn_instance_model.eval()\n",
    "\n",
    "list_models = [pn_ethz_model,\n",
    "               pn_neic_model,\n",
    "               pn_scedc_model,\n",
    "               pn_stead_model,\n",
    "               pn_geofon_model,\n",
    "               pn_instance_model]\n",
    "\n",
    "### Loop over days\n",
    "for i in range(30):\n",
    "    t1 = UTCDateTime(\"2023-12-07\") + i * 86400\n",
    "    ### loop over stations\n",
    "    for net in inventory:\n",
    "        network = net.code\n",
    "\n",
    "        for sta in net:\n",
    "            station = sta.code\n",
    "            \n",
    "            print(network, station, t1)  \n",
    "            \n",
    "            detect_on_fly(network, station, t1, filepath, 6000, 3000, list_models, devcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Associate Picks on Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge picks from all stations\n",
    "csv_list = glob.glob(filepath+'1month/*2023*.csv')\n",
    "all_csv =[]\n",
    "for i in csv_list:\n",
    "    all_csv.append(pd.read_csv(i, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks['time'] = picks['time'].apply(lambda x: (datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%fZ')).timestamp())\n",
    "\n",
    "picks.to_csv(filepath + 'picks_octo.csv', index=False)\n",
    "picks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Station table for PyOcto\n",
    "# client = Client(\"IRIS\")\n",
    "# t1 = UTCDateTime(\"2023-12-01\")\n",
    "# t2 = UTCDateTime(\"2023-12-31\")\n",
    "# inventory = client.get_stations(network=\"AK,AV\", channel=\"BH?\",\n",
    "#                                 starttime=t1, endtime=t2, \n",
    "#                                 maxlatitude=60.1809, minlatitude=58.5911, \n",
    "#                                 maxlongitude=-150.6555, minlongitude=-153.7177)\n",
    "# stations_table = pd.DataFrame(columns=['id','longitude','latitude', 'elevation'])\n",
    "\n",
    "# for net in inventory:\n",
    "#     for sta in net:\n",
    "#         station = sta.code\n",
    "#         temp = pd.DataFrame(data={'id': sta.code,\n",
    "#                                   'longitude': sta.longitude,\n",
    "#                                   'latitude': sta.latitude, \n",
    "#                                   'elevation': sta.elevation},index=[0])\n",
    "#         stations_table = pd.concat([stations_table,temp],ignore_index=True)\n",
    "\n",
    "stations_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "\n",
    "velocity_model = pyocto.VelocityModel0D(\n",
    "    p_velocity=7.0,\n",
    "    s_velocity=4.0,\n",
    "    tolerance=2.0,\n",
    ")\n",
    "# associator = pyocto.OctoAssociator.from_area(\n",
    "#     lat=(57, 61),\n",
    "#     lon=(-155, -149),\n",
    "#     zlim=(0, 200),\n",
    "#     time_before=300,\n",
    "#     velocity_model=velocity_model,\n",
    "#     n_picks=6,\n",
    "#     n_p_picks=3,\n",
    "#     n_s_picks=3,\n",
    "#     n_p_and_s_picks=3,\n",
    "# )\n",
    "\n",
    "# associator = pyocto.OctoAssociator.from_area(\n",
    "#     lat=(57, 61),\n",
    "#     lon=(-155, -149),\n",
    "#     zlim=(0, 200),\n",
    "#     time_before=300,\n",
    "#     velocity_model=velocity_model,\n",
    "#     n_picks=10,\n",
    "#     n_p_picks=5,\n",
    "#     n_s_picks=5,\n",
    "#     n_p_and_s_picks=3,\n",
    "# )\n",
    "\n",
    "# associator = pyocto.OctoAssociator.from_area(\n",
    "#     lat=(57, 61),\n",
    "#     lon=(-155, -149),\n",
    "#     zlim=(0, 200),\n",
    "#     time_before=300,\n",
    "#     velocity_model=velocity_model,\n",
    "#     n_picks=20,\n",
    "#     n_p_picks=2,\n",
    "#     n_s_picks=10,\n",
    "#     n_p_and_s_picks=2,\n",
    "# )\n",
    "\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=10,\n",
    "    n_p_picks=3,\n",
    "    n_s_picks=5,\n",
    "    n_p_and_s_picks=3,\n",
    ")\n",
    "    \n",
    "associator.transform_stations(stations_table)\n",
    "# stations_table.to_csv(filepath + 'stations_table.csv', index=False)\n",
    "stations_table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate picks for these stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(datetime.fromtimestamp)\n",
    "\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo_staonly.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_staonly.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot associated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv(filepath + 'events_detect_octo_staonly.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_staonly.csv')\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DAS picks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAS cable locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cable coordinates from Ethan Williams\n",
    "### We only use Ch. 500-5000\n",
    "kkfls = pd.read_csv('cable_geometry/KKFLS_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)\n",
    "terra = pd.read_csv('cable_geometry/TERRA_coords.xycz',header=None,names=['lon','lat','cha','dep'],delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Picks of DAS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### picks\n",
    "filepaths = ['/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_0_200.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_200_400.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_400_555.hdf5']\n",
    "\n",
    "raw_alldata_picks = np.concatenate([f[\"raw_alldata_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "mul_denoise_picks = np.concatenate([f[\"mul_denoise_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "\n",
    "### recording times\n",
    "record_time_file = '/fd1/QibinShi_data/akdas/qibin_data/recording_times_smaller.csv'\n",
    "df_record_time = pd.read_csv(record_time_file)\n",
    "b_times = [UTCDateTime.strptime(start_t, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') for start_t in df_record_time['record_time'].values]\n",
    "b_times_terra = [b_terra + 0.88 for b_terra in b_times]\n",
    "b_times_kkfls = [b_terra + 1.20 for b_terra in b_times]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge picks with cable locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.05\n",
    "ch_dsamp = 10\n",
    "ch_ind = np.arange(0, 4500, ch_dsamp)\n",
    "len_cat = raw_alldata_picks.shape[0]\n",
    "\n",
    "channel_table = pd.DataFrame(columns=['id','longitude','latitude', 'elevation'])\n",
    "\n",
    "for ch in ch_ind:\n",
    "\n",
    "    if ch >= 2250:  # terra\n",
    "        ch1 = int(ch * 2 - 4000) \n",
    "        longitude = terra.loc[ch1, 'lon']\n",
    "        latitude = terra.loc[ch1, 'lat']\n",
    "        elevation = terra.loc[ch1, 'dep']\n",
    "        b_t = b_times_terra\n",
    "    else:  # kkfls\n",
    "        ch1 = 5000 - ch * 2\n",
    "        longitude = kkfls.loc[ch1, 'lon']\n",
    "        latitude = kkfls.loc[ch1, 'lat']\n",
    "        elevation = kkfls.loc[ch1, 'dep']\n",
    "        b_t = b_times_kkfls\n",
    "\n",
    "    temp = pd.DataFrame(data={'id': 'das'+str(ch),\n",
    "                              'longitude': longitude,\n",
    "                              'latitude': latitude, \n",
    "                              'elevation': elevation},index=[0])\n",
    "    channel_table = pd.concat([channel_table,temp],ignore_index=True)\n",
    "\n",
    "    p_raw = [b_t[i] + raw_alldata_picks[i, ch, 0, 0] if raw_alldata_picks[i, ch, 0, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    s_raw = [b_t[i] + raw_alldata_picks[i, ch, 1, 0] if raw_alldata_picks[i, ch, 1, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    p_den = [b_t[i] + mul_denoise_picks[i, ch, 0, 0] if mul_denoise_picks[i, ch, 0, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    s_den = [b_t[i] + mul_denoise_picks[i, ch, 1, 0] if mul_denoise_picks[i, ch, 1, 1] > thr else np.nan for i in range(len_cat)]\n",
    "    \n",
    "    df_raw = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_raw,\n",
    "        'trace_p_arrival': p_raw\n",
    "    })\n",
    "\n",
    "    df_deno = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_den,\n",
    "        'trace_p_arrival': p_den\n",
    "    })\n",
    "\n",
    "    df_raw.to_csv(filepath+'1month/' + 'CIDAS_Ch_' + str(ch) + '_raw' + '.csv')\n",
    "    df_deno.to_csv(filepath+'1month/' + 'CIDAS_Ch_' + str(ch) + '_deno' + '.csv')\n",
    "    channel_table.to_csv(filepath + 'das_channel_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Associate using DAS and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_model = pyocto.VelocityModel0D(\n",
    "    p_velocity=7.0,\n",
    "    s_velocity=4.0,\n",
    "    tolerance=0.5,\n",
    ")\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -149),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=10,\n",
    "    n_p_picks=0,\n",
    "    n_s_picks=10,\n",
    "    n_p_and_s_picks=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coastal stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_sta = pd.read_csv(filepath + 'picks_octo.csv')\n",
    "sta_table = pd.read_csv(filepath + 'stations_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare DAS picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge picks\n",
    "ch_ind = np.arange(0, 4500, 100)\n",
    "all_csv =[]\n",
    "for i in ch_ind:\n",
    "    filename = filepath+'1month/CIDAS_Ch_' + str(i) + '_deno.csv'\n",
    "    all_csv.append(pd.read_csv(filename, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks_das = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks_das['time'] = picks_das['time'].apply(lambda x: (datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%fZ')).timestamp())\n",
    "\n",
    "picks_das.to_csv(filepath + 'picks_octo_das.csv', index=False)\n",
    "picks_das.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare DAS channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_table = pd.read_csv(filepath + 'das_channel_table.csv')[::10]\n",
    "associator.transform_stations(channel_table)\n",
    "channel_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Sta and DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = pd.concat([picks_sta, picks_das], axis=0, ignore_index=True)\n",
    "stations_table = pd.concat([sta_table, channel_table], axis=0, ignore_index=True)\n",
    "picks\n",
    "picks=picks[picks['phase']=='S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(datetime.fromtimestamp)\n",
    "\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv(filepath + 'events_detect_octo.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments.csv')\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. QC DAS picks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### picks\n",
    "filepaths = ['/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_0_200.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_200_400.hdf5',\n",
    "             '/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/phase_picks_400_555.hdf5']\n",
    "\n",
    "raw_alldata_picks = np.concatenate([f[\"raw_alldata_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "mul_denoise_picks = np.concatenate([f[\"mul_denoise_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "pred_picks = np.concatenate([f[\"predicted_picks\"][:] for f in map(h5py.File, filepaths)])\n",
    "\n",
    "### catalog\n",
    "cat1 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_0_200.xml\")\n",
    "cat2 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_200_400.xml\")\n",
    "cat3 = read_events(\"/fd1/QibinShi_data/akdas/qibin_data/plots_test_picking_dec_ch4500/denoised_catalog_400_555.xml\")\n",
    "cat = cat1 + cat2 + cat3\n",
    "\n",
    "### Recording time\n",
    "df_record_time = pd.read_csv('/fd1/QibinShi_data/akdas/qibin_data/recording_times_smaller.csv')\n",
    "b_times = [UTCDateTime.strptime(start_t, format='decimator2_%Y-%m-%d_%H.%M.%S_UTC.h5') for start_t in df_record_time['record_time'].values]\n",
    "b_times_terra = [b_terra + 0.88 for b_terra in b_times]\n",
    "b_times_kkfls = [b_terra + 1.20 for b_terra in b_times]\n",
    "org_times = [evt.origins[0].time - b_t for evt, b_t in zip(cat, b_times)]\n",
    "pred_picks += np.array(org_times)[:, np.newaxis, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_series(s1, s2, prob, thr=0.05, vmin=0, vmax=60):\n",
    "    offsets = s1-s2\n",
    "    ind = np.where(np.logical_and(np.logical_and(np.logical_and(vmin<s1, s1<vmax), prob > thr), np.fabs(offsets) < 3.0))[0]\n",
    "    ind_rest = np.setdiff1d(np.arange(len(s1)), ind)\n",
    "\n",
    "    s1[ind_rest] = np.nan\n",
    "\n",
    "    return s1, ind, ind_rest\n",
    "\n",
    "qc_picks = np.zeros_like(mul_denoise_picks)\n",
    "\n",
    "for i in tqdm(np.arange(len(raw_alldata_picks))):\n",
    "    \n",
    "    qc_picks[i, :, 1, 0], ind1, ind2 = fit_series(\n",
    "        mul_denoise_picks[i,:,1,0], pred_picks[i,:,1], raw_alldata_picks[i,:,1,1], thr=0.05, vmin=5, vmax=55)\n",
    "    \n",
    "    qc_picks[i, :, 0, 0], ind3, ind4 = fit_series(\n",
    "        mul_denoise_picks[i,:,0,0], pred_picks[i,:,0], raw_alldata_picks[i,:,0,1], thr=0.1, vmin=5, vmax=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge qc picked with DAS locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_dsamp = 10\n",
    "ch_ind = np.arange(0, 4500, ch_dsamp)\n",
    "len_cat = qc_picks.shape[0]\n",
    "\n",
    "for ch in ch_ind:\n",
    "\n",
    "    if ch >= 2250:  # terra\n",
    "        ch1 = int(ch * 2 - 4000) \n",
    "        longitude = terra.loc[ch1, 'lon']\n",
    "        latitude = terra.loc[ch1, 'lat']\n",
    "        elevation = terra.loc[ch1, 'dep']\n",
    "        b_t = b_times_terra\n",
    "    else:  # kkfls\n",
    "        ch1 = 5000 - ch * 2\n",
    "        longitude = kkfls.loc[ch1, 'lon']\n",
    "        latitude = kkfls.loc[ch1, 'lat']\n",
    "        elevation = kkfls.loc[ch1, 'dep']\n",
    "        b_t = b_times_kkfls\n",
    "\n",
    "    p_den = [b_t[i] + qc_picks[i, ch, 0, 0] if qc_picks[i, ch, 0, 0] == qc_picks[i, ch, 0, 0] else np.nan for i in range(len_cat)]\n",
    "    s_den = [b_t[i] + qc_picks[i, ch, 1, 0] if qc_picks[i, ch, 1, 0] == qc_picks[i, ch, 1, 0] else np.nan for i in range(len_cat)]\n",
    "\n",
    "    df_deno = pd.DataFrame({\n",
    "        'event_id': [' '] * len_cat,\n",
    "        'source_type': [' '] * len_cat,\n",
    "        'station_network_code': ['CIDAS'] * len_cat,\n",
    "        'station_channel_code': [' '] * len_cat,\n",
    "        'station_code': ['das'+str(ch)] * len_cat,\n",
    "        'station_location_code': [' '] * len_cat,\n",
    "        'station_latitude_deg': [latitude] * len_cat,\n",
    "        'station_longitude_deg': [longitude] * len_cat,\n",
    "        'station_elevation_m': [elevation] * len_cat,\n",
    "        'trace_name': [' '] * len_cat,\n",
    "        'trace_sampling_rate_hz': [25] * len_cat,\n",
    "        'trace_start_time': b_t,\n",
    "        'trace_S_arrival_sample': [' '] * len_cat,\n",
    "        'trace_P_arrival_sample': [' '] * len_cat,\n",
    "        'trace_S_onset': [' '] * len_cat,\n",
    "        'trace_P_onset': [' '] * len_cat,\n",
    "        'trace_snr_db': [' '] * len_cat,\n",
    "        'trace_s_arrival': s_den,\n",
    "        'trace_p_arrival': p_den\n",
    "    })\n",
    "\n",
    "    df_deno.to_csv(filepath+'1month/' + 'CIDAS_Ch_' + str(ch) + '_deno_qc' + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Octo\n",
    "velocity_model = pyocto.VelocityModel0D(\n",
    "    p_velocity=7.0,\n",
    "    s_velocity=4.0,\n",
    "    tolerance=2.0,\n",
    ")\n",
    "associator = pyocto.OctoAssociator.from_area(\n",
    "    lat=(57, 61),\n",
    "    lon=(-155, -150),\n",
    "    zlim=(0, 200),\n",
    "    time_before=300,\n",
    "    velocity_model=velocity_model,\n",
    "    n_picks=10,\n",
    "    n_p_picks=3,\n",
    "    n_s_picks=5,\n",
    "    n_p_and_s_picks=3,\n",
    ")\n",
    "\n",
    "### DAS channel table\n",
    "channel_table = pd.read_csv(filepath + 'das_channel_table.csv')[::10]\n",
    "associator.transform_stations(channel_table)\n",
    "\n",
    "### DAS picks\n",
    "ch_ind = np.arange(0, 4500, 100)\n",
    "all_csv =[]\n",
    "for i in ch_ind:\n",
    "    filename = filepath+'1month/CIDAS_Ch_' + str(i) + '_deno_qc.csv'\n",
    "    all_csv.append(pd.read_csv(filename, index_col=0))\n",
    "df = pd.concat(all_csv, axis=0)\n",
    "\n",
    "# change to Octo format\n",
    "df.loc[df['trace_p_arrival'].notna(), 'phase'] = \"P\"\n",
    "sta_p_time = df.loc[df['trace_p_arrival'].notna(), ['station_code','phase','trace_p_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_p_arrival\": \"time\"})\n",
    "\n",
    "df.loc[df['trace_s_arrival'].notna(), 'phase'] = \"S\"\n",
    "sta_s_time = df.loc[df['trace_s_arrival'].notna(), ['station_code','phase','trace_s_arrival']].rename(\n",
    "    columns={\"station_code\": \"station\", \"trace_s_arrival\": \"time\"})\n",
    "\n",
    "picks_das = pd.concat(objs = [sta_p_time,sta_s_time] , axis=0)\n",
    "picks_das['time'] = picks_das['time'].apply(lambda x: (datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%fZ')).timestamp())\n",
    "\n",
    "picks_das.to_csv(filepath + 'picks_octo_das_qc.csv', index=False)\n",
    "picks_das\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coastal data\n",
    "picks_sta = pd.read_csv(filepath + 'picks_octo.csv')\n",
    "sta_table = pd.read_csv(filepath + 'stations_table.csv')\n",
    "\n",
    "### Merge DAS and Sta\n",
    "picks_das = pd.read_csv(filepath + 'picks_octo_das_qc.csv')\n",
    "\n",
    "picks = pd.concat([picks_sta, picks_das], axis=0, ignore_index=True)\n",
    "stations_table = pd.concat([sta_table, channel_table], axis=0, ignore_index=True)\n",
    "sta_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association \n",
    "events, assignments = associator.associate(picks, stations_table)\n",
    "\n",
    "associator.transform_events(events)\n",
    "events['time'] = events['time'].apply(datetime.fromtimestamp)\n",
    "\n",
    "all_pick_assignments = pd.merge(events, assignments, left_on=\"idx\", right_on=\"event_idx\", suffixes=(\"\", \"_pick\"))\n",
    "\n",
    "events.to_csv(filepath + 'events_detect_octo_qc1.csv', index=False)\n",
    "all_pick_assignments.to_csv(filepath + 'all_pick_assignments_qc1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv(filepath + 'events_detect_octo_qc1.csv')\n",
    "all_pick_assignments = pd.read_csv(filepath + 'all_pick_assignments_qc1.csv')\n",
    "grid = pygmt.datasets.load_earth_relief(resolution=\"30s\", region=[-155, -150, 57.5, 61])\n",
    "\n",
    "fig = pygmt.Figure()\n",
    "pygmt.config(FONT_LABEL=\"15p,0\", FONT_ANNOT_PRIMARY=\"15p,0\",\n",
    "             FONT_ANNOT_SECONDARY=\"15p,0\", MAP_FRAME_TYPE=\"plain\")\n",
    "pygmt.makecpt(cmap=\"terra\", series=[-7000, 3000])\n",
    "\n",
    "shade = pygmt.grdgradient(grid=grid, azimuth=\"0/300\", normalize=\"e1\")\n",
    "fig.grdimage(grid=grid,shading=shade,projection=\"M10c\",frame=\"a1\",cmap=True)\n",
    "\n",
    "pygmt.makecpt(cmap=\"hot\", series=[0, 150])\n",
    "fig.plot(\n",
    "    x=events[\"longitude\"].values,\n",
    "    y=events[\"latitude\"].values,\n",
    "    size=(events[\"picks\"].values + 20) * 0.005,\n",
    "    fill=events['depth'].values,\n",
    "    cmap=True,\n",
    "    style=\"cc\",\n",
    "    pen=\"black\",\n",
    ")\n",
    "fig.colorbar(position=\"JBC+w10c/0.15c+h\", frame=\"xa50f10+levent depth (km)\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
