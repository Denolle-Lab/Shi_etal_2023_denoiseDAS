{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76858899",
   "metadata": {},
   "source": [
    "# Vision Transformer learning DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3131bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from scipy.signal import filtfilt, butter\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## modules in the package\n",
    "from das_util import try_gpu\n",
    "from das_denoise_models import dataflow_nomask\n",
    "from maevit_model_train import train_augmentation\n",
    "from maevit_model_train import MaskedAutoencoderViT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21b779",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_terra = '/fd1/QibinShi_data/akdas/qibin_data/TERRAtill2023_07_29.hdf5'\n",
    "data_kkfls = '/fd1/QibinShi_data/akdas/qibin_data/KKFLStill2023_07_29.hdf5'\n",
    "with h5py.File(data_terra, 'r') as f:\n",
    "    quake1 = f['quake'][:]\n",
    "with h5py.File(data_kkfls, 'r') as f:\n",
    "    quake2 = f['quake'][:]\n",
    "    \n",
    "sample_rate = 25\n",
    "delta_space = 10\n",
    "ch_num = 7500\n",
    "    \n",
    "tmp = np.append(quake2[:,:ch_num,:], quake1[:,:ch_num,:], axis=0)\n",
    "print(tmp.shape)\n",
    "# %% divide time into 3 windows for smaller image size\n",
    "data = np.reshape(tmp, (tmp.shape[0],ch_num,3,500)).swapaxes(1,2).reshape(-1,ch_num,500)\n",
    "print(data.shape)\n",
    "\n",
    "# %% Pre-filter to suppress strong long-period vibration\n",
    "b, a = butter(4, (0.5, 12), fs=sample_rate, btype='bandpass')\n",
    "filt = filtfilt(b, a, data, axis=2)\n",
    "rawdata = filt / np.std(filt, axis=(1,2), keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f38043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% visualize data\n",
    "time_data = rawdata[5]\n",
    "plt.figure(figsize=(20, 6)); cmap=plt.cm.get_cmap('RdBu'); max_amp = np.median(np.fabs(time_data))\n",
    "x, y=np.arange(time_data.shape[1]), np.arange(time_data.shape[0])\n",
    "plt.pcolormesh(x, y, time_data, shading='auto', vmin=-max_amp, vmax=max_amp, cmap=cmap)\n",
    "plt.xticks(np.arange(0, 1501, 250), np.arange(0, 1501/sample_rate, 250/sample_rate).astype(int))\n",
    "plt.yticks(np.arange(0, ch_num+1, 1000), (np.arange(0, delta_space*(ch_num+1), 1000*delta_space)/1000).astype(int))\n",
    "plt.xlabel(\"Time (s)\", fontsize=20); plt.ylabel(\"X (km)\", fontsize=20)\n",
    "cbr=plt.colorbar(); cbr.set_label('amplitude', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d182aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Shuffle and split dataset\n",
    "X_tr, X, Y_tr, Y = train_test_split(rawdata, rawdata, train_size=0.7, random_state=111)\n",
    "X_va, X_te, Y_va, Y_te = train_test_split(X, Y, train_size=0.5, random_state=121)\n",
    "\n",
    "training_data = dataflow_nomask(X_tr, Nx_sub=500, stride=250)\n",
    "validation_data = dataflow_nomask(X_va, Nx_sub=500, stride=250)\n",
    "test_data = dataflow_nomask(X_te, Nx_sub=500, stride=250)\n",
    "\n",
    "del data, filt, rawdata, tmp, X, Y, X_tr, Y_tr, X_va, Y_va, X_te, Y_te\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6ce2d",
   "metadata": {},
   "source": [
    "## Initialize the vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize the U-net model \"\"\"\n",
    "model = MaskedAutoencoderViT(\n",
    "    img_size=500, patch_size=25, in_chans=1,\n",
    "    embed_dim=2048, depth=16, num_heads=16,\n",
    "    decoder_embed_dim=2048, decoder_depth=16, decoder_num_heads=16,\n",
    "    mlp_ratio=4., norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "            \n",
    "devc = try_gpu(i=1)\n",
    "model = nn.DataParallel(model, device_ids=[1,2,3])  # comment if gpus<4 \n",
    "model.to(devc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09aeac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Hyper-parameters for training\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "train_iter = DataLoader(training_data, batch_size=batch_size, shuffle=False)\n",
    "validate_iter = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# %% Training\n",
    "model, \\\n",
    "avg_train_losses, \\\n",
    "avg_valid_losses = train_augmentation(train_iter,\n",
    "                                   validate_iter,\n",
    "                                   model,\n",
    "                                   optimizer=optimizer,\n",
    "                                   epochs=250,\n",
    "                                   patience=20,\n",
    "                                   device=devc,\n",
    "                                   minimum_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
